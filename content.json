{"pages":[],"posts":[{"title":"","text":"","link":"/2019/07/30/Typora-Hexo를 이용한 Blog관리 Tip/"},{"title":"Typora-Hexo","text":"Typora-Hexo를 이용하여 1시간 안에 Blog 글 올리기! 안녕하세요. 이제 Blog를 본격적으로 시작한지 일주일 밖에 안된 블룸베리입니다. 블로그를 시작하기로 마음먹고, 블로그에 글을 게제하기까지 우여곡절이 많았는데요, 이 글을 읽는 여러분께서는 조금이나마 시간을 아낄 수 있도록 글을 준비해보았습니다. (시간은 소중하니깐요..ㅎㅎ) 그럼 시작하겠습니다! (참고로 이 글은 https://www.stand-firm-peter.me/2018/01/06/Start/ 글을 바탕으로 제가 느낀점을 추가하여 작성한 글입니다!) 1. 블로그 url 정하기제일 먼저, 블로그를 시작하려면 블로그 주소(url)를 받을 블로그 프레임워크를 정해야 해요. 저는 아래 세 블로그 프레임워크를 두고 고민해보았습니다. Tistory Naver Blog Github 각자의 장단점이 있는데요, 저 같이 Tech Blog를 주로 작성하려고 하는 사람은 Github를 활성화해주면 향후 스펙용, 이직 준비용으로 좋아서 Github로 결정하게 됩니다. Github로 블로그 생성하기 궁금하신 분들은 위 링크를 참고해주세요. 이처럼 자신이 블로그 운영을 하는 목적에 따라 블로그 프레임워크를 정하시면 좋을 것 같습니다. 2. 정적 블로그 생성 툴 정하기WordPress와 같이 PHP를 자유자래로 사용하는 컴퓨터 공학도가 아닌 이상, 정적 생성기를 사용하는 것은 필수라고 본다. 정적 생성기로 무엇을 써도 무방하며, 본인이 사용하기 간편하고, contents를 효율적이고 깔끔하게 만들 수 있는 툴을 사용하면 된다. Jekyll : 시도해보았으나, ruby어로 구현된 문법에 친숙하지 못해 이내 포기함. Hexo : 학교 후배가 사용하여 알게됨. node.js를 사용하며, 시작이 간편하고, 기본으로 제공하는 템플릿 적용이 원활하여 이로 결정하게 됨. Gatsby : 웹 개발에 많이 사용하는 react를 사용한다고 함. (잘모름) 3. Markdown Editor 정하기Github 블로그는 모드 textfile의 일종인 markdown 형식을 사용하여 글을 게제한다. 따라서, 효율적인 Markdown Editor 사용이 블로그 글 게시 활동에 핵심이라 해도 과언이 아니다. 두 가지 editor 중에 hotkey가 직관적이라 사용하기 쉽고, Image Insert 혹은 Math module 첨부가 용이한 typora라는 editor를 최종 선택하였다. 3.1 Typora Image Insert 방법Typora에 drag-and-drop으로 Image Insert하기 위해서는 몇 가지 typora setting을 건드려야 한다. 상대 경로 활성화 하기 좌측 상단에 ‘File-Preference-Images Insert 항목에서 ‘Use relative path if possible‘ 체크 박스를 활성화 한다. Copy Image file to folder 활성화 하기 이게 뭐하는거냐? Github에 사진을 업로드 하기 위해서는 반드시 서버 상에도 그 이미지를 업로드 해야 한다. 다시 말해, 단순히 typora editor에 drag-and-drop한다고 해서 markdown 파일에 그림이 첨부되는게 아니다. 좀 복작하지만, 비슷한 효과(?)를 내기 위해 drag-and-drop했을 경우 설정된 경로로 drop한 이미지가 복사되는 기능을 활성화 한 것이라 볼 수 있다. 방법은 아래와 같다. ‘Edit - Image Tools - When Inesrt Local Image - Copy Image File to Folder’ 체크 ​ 저장 경로 설정하기 제일 중요한 부분이다! 이것 때문에 주말에 5시간동안 삽질을 했다.. 부디 그런일 마시길. 저장 경로를 이해하기 위해서 먼저 Hexo의 _config.yml파일을 살펴봐야 한다. Github서버에 Markdown파일을 업로드하면 _config.yml파일에 permalink에서 지정한 형식으로 경로가 설정된다. 예를 들어 ‘Mobilenetv3.md’파일을 ‘나의 directory/source/Mobilenetv3.md ‘라는 곳에 생성했다고 하자. 실제로 deploy를 하게되면 github Web 주소 상에서는 ‘https://BloomBerry.github.io/ree/master/2019/07/28/Mobilenetv3' 라는 주소로 index.html파일이 생성된다. 이때 중요한 것은 동일한 경로, 예컨데 ‘나의 directory/source’에 Mobilenetv3라는 디렉토리를 만들고 그곳에 이미지를 넣는다고 생각하면 오산이다. Hexo는 themes 디렉토리 내부 경로만 참조하기 때문에 ‘themes/자신의 themes’ 경로에 이미지를 추가해줘야 한다. 예컨데 icarus를 사용한다면 ‘themes/icaurs/source/MobilenetV3’ 경로에 image를 추가해야 한다. 이를 원활하게 하기 위해 typora-root-url과 typora-copy-images-to를 이용하면 drag-and-drop하여 Image를 추가할 수 있다. 주의할 점은 github에 deploy하기 전에 반드시 두 줄을 #으로 주석처리해야 한다. [Optional] 글 별로 이미지를 관리하고 싶을 때 개인의 취향에 맞기겠지만, markdown별로 첨부한 이미지를 관리하고 싶을 때는 _config.yml에 post_asset_folder를 True로 만들어 주면 된다. command 창에 hexo new New_Title.md라고 치면 typora-copy-images-to에서 설정한 경로에 md파일명과 동일한 디렉토리가 생성되는 것을 볼 수 있다. 짜잔! 이 글역시 위 방법으로 만든 글이다. 이 글을 작성하는데 1시간 정도밖에 걸리지 않았다. 놀랍군!!","link":"/2019/07/30/Typora-Hexo/"},{"title":"Mobilenetv3","text":"Searching for Mobilenet V32019.05.09 Andrew Howard, et. al. from Google AI AbstractMobilenet V3는 hardware-aware NAS (Neural Architecture Search)기법인 NetAdapt을 사용하여 타겟하는 Mobile Device CPU에서 Latency를 줄임과 동시에 성능을 끌어올릴 수 있음을 보였다는데 의의가 있다. 본 논문에서는 Mobilenet V3를 Mobilenet V3-Large와 MobilenetV3-Small 두 가지 버전으로 나누어 Classification, Object Detection과 Semantic Segmentation을 수행하였다. Segmantic Segmentation의 경우는 LR-SPP (Lite Reduced-Spatial Pyramid Pooling)라는 새로운 Decoder구조를 제안하였다. 실험 결과 MobilenetV3-Lage의 Classification은 MobilenetV2에 비해 ImageNet에서 Top-1 accuracy가 3.2% 향상됨과 동시에 Latency가 15% 감소하였다.(MobilenetV3-Small은 accuracy 4.6% 감소) 1. Introduction이 논문은 모바일 디바이스 상에서 속도-성능간 trade-off를 최적화하기 위한 방안들을 다룬다. Complementary Search Engine : Latency &amp; Accuracy를 목표값으로 NAS (Neural Architecture Search)를 진행함. 모바일 셋팅에 최적화된 형태의 비선형 함수 H-swish 함수 사용 새로운 network 구조 제안 : Inverted Resudial + Squeeze-and-Excite 새로운 Segmentation Decoder (LR-SPP) 2. Related WorkAccuracy-Latency 최적화를 위한 다양한 연구 결과를 요약하면 다음과 같다. SqueezeNet : 1x1 conv로 parameter 수를 줄임 Mobilenet V1 : Depthwise Separable conv로 channel-wise 곱셈 paremeter 를 줄임 Mobilenet V2 : Inverted Residual과 Linear Bottleneck을 사용하여 모바일 디바이스 상에서 효율적인 구조 채택 ShuffleNet : Group conv + Channel shuffle을 사용하여 MAC 를 줄임 CondenseNet : Group Conv에 사용될 connections를 학습함 ShiftNet : Shift operation을 point-wise conv와 사용하여 계산량이 많은 Spatial conv 대체 3. Efficient Mobile Building BlocksMnasNet은 Mobilenet V2의 bottleneck 기본 뼈대에서 squeeze-and-excite구조와 light attention module을 추가한 구조를 채택하고 있다. Mobilenet V3는 MnasNet구조에서 swish non-linearity로 업데이트하였으며, fixed-point arithmetic구조에서 비효율적인 sigmoid를 hard sigmoid 구조를 채택하였다. 4. Network SearchNetwork Search는 network의 최적 Block를 찾는데 Platform-aware NAS와 Layer 수 최적화를 위한 NetAdapt 알고리즘을 사용한다. 4.1 Platform-Aware NAS for Block-wise SearchPlatform-Aware NAS는 [43] MnasNet: Platform-Aware Nerual Architecture Search for Mobile 에서 다루고 있는 다중목적 보상함수를 사용하고 있다.$$ACC[m]×[LAT[m]/TAR]^w$$위 식은 Parto-optimal solution을 최적화하기 위해 Target Latency [TAR]항을 분모로 모델 m에 대하여 Accuracy [ACC[m]]와 Latency[LAT[m]] 항을 사용한다. *w은 ACC와 *LAT 간의 비율을 조정하는 항이다. 기존 논문 [43]에서는 w=-0.07를 사용하였지만, 본 논문에서는 작은 모델에 최적화하도록 LAT\\에 weight를 더 주기 위해 w=-0.15를 주어 모델 m을 찾는다. 4.2 NetAdapt for Layer-wise SearchNetAdapt는 *[46] Netadapt: Platform-Aware neural network adaptation for mobile applications*에서 다루고 있다. 요약하면, 4.1절에서 다중목적 보상 NAS로 찾은 seed architecture로 시작한다. 각 step에 대해서 새로운 proposal을 제안한다. 이때, proposal은 이전 step에서의 model에 비해 latency가 최소한계치 δ 만큼 감소시켜야 한다. δ=0.01L L= seed model의 Latency Pretrained weight 값으로 이전 step에서 사용한 model 중 truncated된 부분을 제외한 부분을 채택하며, 새로 생성된 layer는 randomly initialized시킨다. Proposal한 model들을 T step만큼 fine tuning을 거치고 accuracy를 생성한다. T=10,000를 사용 Evaluation Metric을 통해 proposal된 model들 중 최고의 proposal model을 찾는다. Eval Metric =max $$Δ|ACC|\\overΔLAT$$ Insight: Proposal이 discrete하기 때문에 trade-off curve slope를 최대화 함 Proposal 시 변경 가능한 요소들 Expansion Layer 크기 Bottleneck 크기 Target Latency에 도달할 때까지 1~2 과정을 반복한다. 5. Network ImprovementsNetwork Search로 찾은 모델을 더 개선하기 위해 계산량이 많은 Layer를 변경하고, Quantization frinedly한 H-swish 비선형 함수를 제안한다. 5.1 Redesigning Expensive LayersN기존 Network Search로 찾은 모델은 시작, 끝 Layer에서 유독 많은 계산량을 보였다. 이는 기존 Network Search Method의 search space를 벗어나는 부분으로, 수동 tuning을 하여 Accuracy는 불변하고, Latency를 줄여주었다. Final Layer 7×7 Conv Layer into Average pooling Layer Same Accuracy 10 ms Latency (15% of running time) &amp; 30MAdds drop Initial Layer channel 32 channel into 16 channel Non-linearity Function : swish or ReLU Same Accuracy 3 ms Latency &amp; 10MAdds drop 5.2 Nonlinearitiesswish는 *[36] Searching for Activation Function*에서 제안한 비선형 함수로 ReLU를 대체하여 neural network의 accuracy를 향상시킨다. 이때, sigmoid σ(x)를 사용하기에 계산량의 증가가 있다.$$swish(x)=x⋅σ(x)$$sigmoid 함수 σ(x)의 계산량을 모바일 디바이스 상에서 줄이기 위한 두 가지 방법을 제안한다. h-swish $$h−swish[x]=x\\frac{ReLU6(x+3)}6$$ ReLU6는 어떤 Hardware / Software platform에서도 최적화가 가능하다. Framework마다 Sigmoid 구현하는데 따른 numerical precision loss를 없앨 수 있다. Quantized Sigmoid조차도 ReLU보다 느리다. 위 세 가지 이유로, sigmoid 함수를 ReLU6로 바꾼다. h-swish 후 배치 비선형 함수로 인한 Latency는 뒤로 갈 수록 feature-map의 크기가 줄어듦에 따라 감소한다. 따라서, h-swish를 network의 뒤에만 배치하는 것이 더 효과적임을 실험적으로 구했다. 위에 언급한 최적화를 사용함에도 불구하고, h-swish는 여전히 latency가 존재한다. 그럼에도 accuracy-latency tradeoff가 긍정적이고, 향후 software optimize에 따른 성능 향상을 염두하여 Network의 뒷부분에 H-swish 비선형 함수를 사용한다. ReLU와 같은 linear 함수 사용 시, latency bottleneck은 메모리 접근 시 발생함. 효과적으로 메모리 배치 시 최적화 가능함. 5.2.1. Large Squeeze-and-Excite MnasNet 논문에서는 Squeeze-and-excite의 channel은 convolution layer의 채널 수에 상대적이었다. 하지만, 본 논문에서는 Expansion Layer의 1/4 사이즈로 고정시켰다. 이로 인한 accuracy는 향상한 반면, parameter는 소폭만 증가헀다. 5.3 Mobilenet V3 DefinitionMobilenet V3 - Large 구조는 아래와 같다. (Mobilenet-Small은 위 Table 2 참고)* 6. Experiments 비교 대상: Accuracy vs. Latecny &amp; MAdds (Multiply adds) Training setup 학습 환경 : 4x4 TPU Pod Optimizer : RMSPropOptimizer with 0.9 Momentum (Tensorflow) Batch size: 4,096 (128 images per chip) Learning Rate 스케줄 : 0.01 initial rate of decaying 0.01 every 3 step dropout : 0.8 l2 weight decay 1e-5 Exponential moving Average with 0.9999 decay Batch Norm : average decay of 0.99 Latency Measurement Hardware: Google Pixel phones Framework: Tensorflow Lite Benchmark Tool Core 갯수: Single 6.1 Classification Dataset: ImageNet 6.2 Result Mobilenet V3-Large 1.0 성능(Accuracy) &gt; SOTA Mobile Device Target 모델 Mobilenet V3-Small 0.75 지연시간(Latency) &lt; SOTA Mobile Device Target 모델 이미지 해상도(Resolution)이 큰 Mobilenet-V3-Small이 Mobilenet-V3-Large보다 미세하게 성능이 우수함. 하지만, 이미지 해상도는 보통 결정된 값이기에 큰 의미는 없는듯 6.2.1 Ablation Study 전체 Network를 H-swish로 대체하면 성능-속도 trade-off가 좋지 않음 성능 증가 (0.9%) 속도 감소 (-32%) Network 뒷부분만 교체하면 성능-속도 trade-off가 조아짐 성능 증가(0.7%) 속도 감소 (-12%) 크게 의미 없는 수치같음… 6.2 Detection Datset: MsCOCO OD Architecture : SSD Lite (OS 16 ~ OS256) OS 16 (C4) MobilenetV3-Lage 13th bottleneck block MobileentV3-Small 9th bottleneck block OS 32 (C5) After pooling layer (Small &amp; Large) Reduced Channel into Half ImageNet의 클래스 수 (1,000)에 비해 MsCOCO의 클래스 수 (80)이 작기에 기존 channel 수는 redundunt하다고 여김 실제로 성능하락 없이 속도 향상(15%)만 가져옴 Mobilent V2와 동일한 성능 (22.0 mAP vs. 22.1 mAP)이면서 빠른 속도 (150 ms vs. 200 ms) 25% 속도 향상 보임 6.4 Semantic Segmentation Dataset : CityScapes Fine Images (mIOU) Pretraiend Model 사용 x Channel 절반 사용 (OD와 동일한 이유) CityScapes 클래스 19 &lt;&lt; ImageNet 클래스 1,000 LR-ASPP (Lite Reduced design of Atrous Spatial Pyramid Pooling module) 1x1 conv + Global Average Pooling Layer (+ Squeeze-and-Excite) Pooling Layer에 더 큰 stride, kernerl을 사용함 마지막 Layer에 Atrous conv.와 skip connection을 사용함. Mobilenet V3-Large: ESPNet v2, CCC2, ESPNetv1보다 각각 10.5%, 10.6%, 12.3% 성능 향상 + 무게 감소 (Madds) Mobilenet V3-Small: ESPNet v2, CCC2, ESPNetv1보다 각각 1.7, 1.59, 2.24 배 속도 향상 + 6.2% 성능 향상 7. Conclusion Classificaiton, Object Detection, Semantic Segmentation 모두 좋은 결과를 보임 Mobile model의 차세대 Network 제시 비선형 함수 h-swish 사용 양자화에 친화적인 Squeeze-and-Excite 사용 LR-ASPP 선보임.","link":"/2019/07/28/Mobilenetv3/"}],"tags":[{"name":"Static Generator, Hexo, Typora, Blog","slug":"Static-Generator-Hexo-Typora-Blog","link":"/tags/Static-Generator-Hexo-Typora-Blog/"},{"name":"Mobilent v3, AI, Object Detection, Quantization, Tensorflow Lite","slug":"Mobilent-v3-AI-Object-Detection-Quantization-Tensorflow-Lite","link":"/tags/Mobilent-v3-AI-Object-Detection-Quantization-Tensorflow-Lite/"}],"categories":[]}