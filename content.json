{"pages":[],"posts":[{"title":"Typora-Hexo","text":"Typora-Hexo를 이용하여 1시간 안에 Blog 글 올리기! 안녕하세요. 이제 Blog를 본격적으로 시작한지 일주일 밖에 안된 블룸베리입니다. 블로그를 시작하기로 마음먹고, 블로그에 글을 게제하기까지 우여곡절이 많았는데요, 이 글을 읽는 여러분께서는 조금이나마 시간을 아낄 수 있도록 글을 준비해보았습니다. (시간은 소중하니깐요..ㅎㅎ) 그럼 시작하겠습니다! (참고로 이 글은 https://www.stand-firm-peter.me/2018/01/06/Start/ 글을 바탕으로 제가 느낀점을 추가하여 작성한 글입니다!) 1. 블로그 url 정하기제일 먼저, 블로그를 시작하려면 블로그 주소(url)를 받을 블로그 프레임워크를 정해야 해요. 저는 아래 세 블로그 프레임워크를 두고 고민해보았습니다. Tistory Naver Blog Github 각자의 장단점이 있는데요, 저 같이 Tech Blog를 주로 작성하려고 하는 사람은 Github를 활성화해주면 향후 스펙용, 이직 준비용으로 좋아서 Github로 결정하게 됩니다. Github로 블로그 생성하기 궁금하신 분들은 위 링크를 참고해주세요. 이처럼 자신이 블로그 운영을 하는 목적에 따라 블로그 프레임워크를 정하시면 좋을 것 같습니다. 2. 정적 블로그 생성 툴 정하기WordPress와 같이 PHP를 자유자래로 사용하는 컴퓨터 공학도가 아닌 이상, 정적 생성기를 사용하는 것은 필수라고 본다. 정적 생성기로 무엇을 써도 무방하며, 본인이 사용하기 간편하고, contents를 효율적이고 깔끔하게 만들 수 있는 툴을 사용하면 된다. Jekyll : 시도해보았으나, ruby어로 구현된 문법에 친숙하지 못해 이내 포기함. Hexo : 학교 후배가 사용하여 알게됨. node.js를 사용하며, 시작이 간편하고, 기본으로 제공하는 템플릿 적용이 원활하여 이로 결정하게 됨. Gatsby : 웹 개발에 많이 사용하는 react를 사용한다고 함. (잘모름) 3. Markdown Editor 정하기Github 블로그는 모드 textfile의 일종인 markdown 형식을 사용하여 글을 게제한다. 따라서, 효율적인 Markdown Editor 사용이 블로그 글 게시 활동에 핵심이라 해도 과언이 아니다. 두 가지 editor 중에 hotkey가 직관적이라 사용하기 쉽고, Image Insert 혹은 Math module 첨부가 용이한 typora라는 editor를 최종 선택하였다. 3.1 Typora Image Insert 방법Typora에 drag-and-drop으로 Image Insert하기 위해서는 몇 가지 typora setting을 건드려야 한다. 상대 경로 활성화 하기 좌측 상단에 ‘File-Preference-Images Insert 항목에서 ‘Use relative path if possible‘ 체크 박스를 활성화 한다. Copy Image file to folder 활성화 하기 이게 뭐하는거냐? Github에 사진을 업로드 하기 위해서는 반드시 서버 상에도 그 이미지를 업로드 해야 한다. 다시 말해, 단순히 typora editor에 drag-and-drop한다고 해서 markdown 파일에 그림이 첨부되는게 아니다. 좀 복작하지만, 비슷한 효과(?)를 내기 위해 drag-and-drop했을 경우 설정된 경로로 drop한 이미지가 복사되는 기능을 활성화 한 것이라 볼 수 있다. 방법은 아래와 같다. ‘Edit - Image Tools - When Inesrt Local Image - Copy Image File to Folder’ 체크 ​ 저장 경로 설정하기 제일 중요한 부분이다! 이것 때문에 주말에 5시간동안 삽질을 했다.. 부디 그런일 마시길. 저장 경로를 이해하기 위해서 먼저 Hexo의 _config.yml파일을 살펴봐야 한다. Github서버에 Markdown파일을 업로드하면 _config.yml파일에 permalink에서 지정한 형식으로 경로가 설정된다. 예를 들어 ‘Mobilenetv3.md’파일을 ‘나의 directory/source/Mobilenetv3.md ‘라는 곳에 생성했다고 하자. 실제로 deploy를 하게되면 github Web 주소 상에서는 https://BloomBerry.github.io/2019/07/28/Mobilenetv3 라는 주소로 index.html파일이 생성된다. 이때 중요한 것은 동일한 경로, 예컨데 ‘나의 directory/source’에 Mobilenetv3라는 디렉토리를 만들고 그곳에 이미지를 넣는다고 생각하면 오산이다. Hexo는 themes 디렉토리 내부 경로만 참조하기 때문에 ‘themes/자신의 themes’ 경로에 이미지를 추가해줘야 한다. 예컨데 icarus를 사용한다면 ‘themes/icaurs/source/MobilenetV3’ 경로에 image를 추가해야 한다. 이를 원활하게 하기 위해 typora-root-url과 typora-copy-images-to를 이용하면 drag-and-drop하여 Image를 추가할 수 있다. 주의할 점은 github에 deploy하기 전에 반드시 두 줄을 #으로 주석처리해야 한다. [Optional] 글 별로 이미지를 관리하고 싶을 때 개인의 취향에 맞기겠지만, markdown별로 첨부한 이미지를 관리하고 싶을 때는 _config.yml에 post_asset_folder를 True로 만들어 주면 된다. command 창에 hexo new New_Title.md라고 치면 typora-copy-images-to에서 설정한 경로에 md파일명과 동일한 디렉토리가 생성되는 것을 볼 수 있다. 짜잔! 이 글역시 위 방법으로 만든 글이다. 이 글을 작성하는데 1시간 정도밖에 걸리지 않았다. 놀랍군!!","link":"/2019/07/30/Typora-Hexo/"},{"title":"YOLO v2","text":"YOLO v20. Background Archive: ‘16.12 CVPR 2017 Joesph Redmon Abstract SOTA training 기법 적용 (Multi-scale training) Fast YOLO v2: 67 FPS, 76.8 mAP (YOLO v1: 45 FPS, 63.4 mAP) YOLO v2: 40FPS, 78.6 mAP &gt;&gt; Better than SSD, FR-RCNN ImageNet 200개 Class 중 44개의 class label만 가지고 19.3mAP –&gt; Unlabelled class (156개) 1. Introduction 일반화 가능한 물체 인식 알고리즘의 조건 : 빠르고, 정확하며, 수 많은 물체를 분류 가능해야 함. Classfication Label 비용 &lt;&lt; Detection Label 비용 –&gt; 현실적으로 수 많은 물체에 대한 label data 확보 어려움 새로운 학습 기법 제안 : Joint Training + Dataset Combination Classifjication label을 계층적으로 물체를 분류함 기존에 수 많은 Classfication Data와 Object Detection Data를 함께 사용함 Classification Data는 vocabulary를 늘리고 robustness를 높여주기 위함. 2. Better Recall과 localization error가 여전히 RPN계열에 비해 낮음. 이 두 가지에 집중하여 네트워크 개발함 Batch norm: 다른 Regularization 방법 (ex. Drop Out)을 없애도 됨. mAP 약 2% 상승 High Resolution Classifier: ImageNet Pretrain 시, 이미지 해상도를 2배 증가 (224 –&gt; 448)시켜 10에폭 돌림. mAP 약 4% 상승 Grid cell의 OD Head를 FC Layer에서 Convolution Layer로 변경함 물체의 위치 좌표를 직접 찾지 않고, Anchor Box간의 offset을 구함 (FR-CNN 따라함) 448 –&gt; 416으로 변경함. ($\\because$ 홀수로 두어, 중심점이 생기도록 하기 위함) 최종 Feature Map 크기는 13x13 Anchor Box 갯수 98 에서 &gt;1,000 로 많아짐 Anchor Box를 사용 : mAP (69.5 –&gt; 69.2) / Recall (81% –&gt; 88%) Anchor Box Size : K-mean clustering 이용함 K값에 따라 계산량 vs 성능 trade-off가 있음. 기존에 FR-CNN의 Hand-picked Anchor Box보다 우수함. Dimension Prediction: SSD, RPN계열이 쓰는 Offset을 추정하는 방식 ($\\Delta x, \\Delta y$) 과 달리, grid cell내로 제한된 영역에서 grid cell과의 상대적 위치 x, y값을 직접 추정함. Offset 방식: 초기에 불확실성이 있음 ($\\because $이미지 내 어디든 존재 가능) Grid cell방식: Anchor Box의 중심점을 grid cell별로 제한함으로 학습이 용이해짐. 성능이 약 5% mAP 상승 Fine Grained Features: 13x13 Feature Map에 26x26 Feature Map을 Concat하여 박스를 추정함. 1% mAP 상승 개인적으론 Feature Map의 Local한 정보를 살리지 못하는 방식이라고 생각됨. (특허 회피 같은 느낌?) Multi-Scale Training: 매 10 에폭마다 {320, 352, .., 608}의 이미지 사이즈 중 랜덤으로 입력 이미지 크기를 resize하여 학습. 효과: 다양한 해상도의 이미지로 학습함 –&gt; 이미지 해상도에 Robust?? 288x288의 경우 90 FPS 608x80x의 경우 mAP &gt; FR-RCNN mAP Further Experiments: Pascal VOC12, COCO eval 결과 모두 SOTA 동등 이상 성능 COCO의 경우, 물체의 크기의 분산이 큼 –&gt; K-clustering 효과 떨어짐으로 SSD에 비해 성능 저하 되는 것으로 사료? 3. Faster Darknet-19: Yolo v2에서 사용한 base network. (5.58 BOPs, 72.9%/91.2% Top-1/5 Accuracy) VGG-16: (30.69 BOPs, 90% Top-5 Accuracy) 3x3 Conv + 1x1 Conv, BN, Global Avg Pool, Channel x2 After Pooling, etc Classification Network 학습 : 224x224로 학습 후, 448x448로 10 에폭 학습 Detection Network 학습 : Classification Network에서 Final Conv Layer만 제거 후, 3개의 3x3 Conv w/ 1,024 Channel Layer 추가 + 1x1 Conv Layer 추가 4. Stronger Detection과 Classification 전용 Dataset을 Jointly 학습하는 기법 제안함. Classification 레이블 데이터일 경우, Classification 관련 구조만 back-prop. 학습 Detection 레이블 데이터일 경우, 전체 구조 back-prop. 학습 Multi Label 기법 사용 ($\\because$Detection Label, Classification Label 상호 베타적이지 않음. ex. ‘Dog’ ‘요크셔테리어’) 계층적 구조를 띈 ‘WordTree’ 라벨링 제안 $Pr(Norfolk terrier) = Pr(Norfolk terrier|terrier) * Pr(terrier | hunting dog) … Pr(mammal | animal) * Pr(animal | object)$ Classification 데이터에서 $Pr(Object)=1$ Multi Labelling Detection 데이터는 YOLO v2의 Objectness score를 활용. Depth Search Method로 최대 score &amp; threshold 넘는 layer까지 레이블을 취함 COCO + Imagenet Combination WordTree를 이용하여 mapping함 ImageNet의 9,000 class + MS COCO 80 class 데이터 모두 사용함 MS COCO 데이터를 over-sampling하여 비율을 4:1로 맞춰 학습함. Anchor prior는 3개만 사용 (output의 사이즈를 고려함) ImageNet Detection Task: (200개 class 중 44개만 학습에 사용됨.) 19.7 mAP (156개 에 대해서는 15.6 mAP) Weakly-supervised 방식임 여전히 실시간 보장함 동물 종류는 잘 구분하나, ‘cloth’, ‘sunglasses’ 등 옷 종류는 잘 구분 못함. 옷 종류가 구분하기 어려워 그런 것으로 사료됨 5. Conclusion Detection &amp; Classification 데이터를 jointly 활용하는 Weakly-supervised 학습기법 제안 비단 Detection Task뿐 아니라, Semantic Segmentation Task에도 활용할 예정 ImageNet + COCO 데이터셋을 이용해 실시간, 범용 물체인식 알고리즘 YOLO V2 제안 Multi-scale Training 등 여러 학습기법 제안","link":"/2019/10/10/YOLO v2/"},{"title":"","text":"YOLO (You Look Only Once)0. Background‘15.06 Archive ‘16.05 CVPR Joseph Redmon, et. al. Abstract 물체 인식 문제를 클래스 정보를 포함한 앵커 박스를 공간적으로 분리된 관점에서 회기 문제로 봄. Single Network 관점에서 end-to-end로 학습하여 속도가 빠르고 최적화가 가능함. (Titan X: 45 FPS) Fast Yolo는 155 FPS SOTA인 R-CNN에 비해 localization 에러는 많으나, False Positive가 적다. 당시 SOTA였던 DPM, R-CNN에 비해 일반화 효과가 뛰어남. 1. Introduction Deformable Parts Model (DPM)은 Sliding Window를 이용하여 모든 image에 대해 균일한 stride로 움직인다. (비효율적) RCNN은 Region Proposal을 통과한 후, potential Bounding Box에 대해 Classifier를 통과시키고, post-processing을 통과시킨다. 각각의 파트가 따로 학습되어, 학습이 용이하지 않고, 최적화가 어렵다. YOLO은 end-to-end로 한 번에 이미지 pixel domain에서 Box coordinate를 출력시킨다. 이미지 Resize –&gt; Single CNN 통과 –&gt; Confidence 값을 기준치로 filtering 다른 OD에 비해 장점 학습이 용이하다. [Figure 1] 물체 인식 문제 –&gt; 회귀 문제로 봤음. (Simple Pipeline) Titan X 기준 45 FPS –&gt; 1 이미지 처리당 &lt; 25 [ms] latency –&gt; real time Global Context를 encode 함 –&gt; Background False Positive 줄어듦. DPM이나 R-CNN은 지엽적인 정보만 봄 –&gt; YOLO에 비해 Background에 대한 FP가 2배 이상 나타남 일반화 효과가 뛰어남 자연의 이미지에서 학습하고 Artwork으로 test하면, 다른 OD Network에 비해 좋은 성능이 나옴. 새로운 Domain Data나 기대하지 않은 image에 더 좋은 성능이 날 것임 2. Unified Detection R-CNN에서 3부분이 담당하던 기능을 Single DNN으로 일원화함 전체 이미지에서 처리한 feature를 사용하여 bounding box를 예측함 Real-time end-to-end Training (학습의 용이성) High Precision (당시 real-time Detector보다 2배 이상 성능) S x S grid를 기준으로 물체와의 중심점의 차이값 ($\\Delta x, \\Delta y$), 전체 이미지에 대한 물체의 크기 ($w, h$) , 그리고 물체인 정도를 G.T. box와 grid의 IOU를 가지고 판단함. $$Pr(Class|Object) * Pr(Object) * IOU_{pred}^{truth} =Pr(Class)*IOU_{pred}^{thres}$$ 각 Grid 마다 B개의 Bounding Box를 생성함. (B=2 for YOLO v1) Objectness Score는 GT와 Predicted Box간의 IOU를 가지고 판단함. ( if Objectness &gt; thres ? true : false ) Probability는 B와 무관하게 Grid마다 1개씩만 추측한다. 최종 Output은 $$S \\times S\\times(B*5+C)$$이다. Pascal VOC에서 C=20, B=2, S=7을 사용함. 2-1. Network Design 앞 단은 Conv Layer + 뒷 단은 FC Layer로 구성 YOLO는 24개의 Conv Layer, Fast YOLO는 9개의 Conv Layer로 구성 GoogleNet를 참조하여 Network를 Design. (단, Inception모듈 대신 Simple하게 3x3 Conv 뒤에 1x1 Conv를 둠) 2-2. Training ImageNet Pretrained Weight 사용 ( Top 1 accuracy: 88%) Input Image : 224 x 224 –&gt; 448 x 448로 바꿈 ($\\because$Image resolution이 높아야 precision이 향상) $w, h$는 Image의 width, height로 정규화 ($\\because$0~1 사이로 두기 위해) $x, y$는 grid cell과의 offset으로 paramitarized Leaky Relu 사용 Loss Function $|_i$ 는 i번째 grid cell을 의미 i번째 grid cell에 물체가 있는 경우만 classification penalty부여 $|_{ij}^{obj}$ 는 i번째 grid cell의 j번째 bounding box를 의미 i번째 grid cell j번째 bounding box가 물체가 있는 경우 localization penalty 부여 $\\lambda_{coord}=5, \\lambda_{noobj}=0.5$ 사용 localization error에 가중치를 줌으로써 noobj에 따른 gradient decent=0수렴 현상 방지 2-3. Inference 98개의 Box만 생성 크기가 큰 물체, grid cell의 경계면에 맞닿아 있는 물체의 경우 여러개의 grid로부터 물체검출이 됨. Non-Maximum Suppression을 통해 약 2~3% mAP 향상 가능 2-4. Limitations of YOLO Grid cell당 2개의 물체, 1개의 클래스만 검출가능하기 때문에, 작고 군집된 물체를 잡는데 약함. 물체의 모양을 데이터 기반으로 학습하다 보니, Aspect Ratio (가로세로 비율), 물체의 형태가 새로운 물체의 경우 일반화 효과가 떨어짐. Feature가 coarse하다 보니, 물체의 Localized resolution이 좋지 못함. 큰 물체, 작은 물체 모두 동일하게 작은 에러에 대한 loss를 부여함. 이 때문에 local error 발생 (IOU를 가지고 loss를 잡는게 더 좋을 것) 3. Comparison to Other Detection Systems Detection System은 모두 Robust한 Feature를 추출하고, Localizer 혹은 Classifier를 feature space를 통과시킨다. Sliding Window로 전체 Feature를 훑거나, 특정 region만 본다. Deformable Parts Model (DPM): Feature 추출, 영역 분류, 높은 score의 영역으로부터 BBox 검출 등의 단계를 disjoint하게 실행함 R-CNN: Selective Search를 이용하여 잠재적인 BBox 2,000개를 생성함. SVM은 박스 score를 내고, NMS 통과시킴 Other Fast Detectors: Selective Search를 Region Proposal로 대체함. Computation 공유 Deep MultiBox: DNN을 이용한 Single Object Detection에 사용됨. ROI를 추론함. Class prediction (probability X) OverFeat: Sliding Window를 사용하여 DNN으로 Localization을 수행. Global context 사용 못하는 한계 MultiGrasp: Grid 방식 도입. DNN으로 Single Object 검출에 사용. Class 추출 X, Multi Object X 4. Experiments Pascal VOC 2007, 2012를 Fast-RCNN과 비교하며 실험 진행 4-1. Comparison to Other Real-Time Systems mAP &amp; speed Trade-off를 살펴봄 Fastest DPM: Selective Search를 static BBox proposal로 대체함 Real-time(30FPS) 만족하는 Detector중에서 최고성능임 Faster R-CNN은 YOLO에 비해 10 mAP가 좋으나, 실시간성이 보장이 안됨. 4-2. VOC 2007 Error Analysis Fast R-CNN vs. YOLO in Pascal VOC 2007 Yolo가 Localization error가 많았음 Fast R-CNN이 YOLO에 비해 False Positive가 약 3배 높음 Correct: correct class &amp; IOU &gt; 0.5 Localization: correct class &amp; 0.1 &lt; IOU &lt; 0.5 Similar : class is similar &amp; IOU &gt; 0.1 Other: class is wrong &amp; IOU &gt; 0.1 Background: IOU &lt; 0.1 for any object 4-3. Combining Fast R-CNN and YOLO YOLO와 Fast-RCNN의 Ensemble 결과, Fast-RCNN은 약 2~3% mAP 향상이 있었음 이는 BaseNet만 바꾼 Fast-RCNN간의 Ensemble보다 월등히 향상된 결과임 ($\\because$YOLO는 Fast-RCNN과 다른 error 양상을 보이기 때문임) 4-4. VOC 2012 Results Fast-RCNN+YOLO조합은 ‘15.11월 당시 Top-4 달성함 4-5. Generalizability: Person Detection in Artwork Fast-RCNN은 AP가 많이 낮아짐($\\because$Selective search로 local context만 봄) 이에 반해 YOLO는 AP가 적게 낮아짐 ($\\because$global context를 봄) 5. Real-Time Detection In The Wild 6. Conclusion 실시간을 보장하는 가장 빠른 일반화된 물체 인식 알고리즘임 http://pjreddie.com/yolo/ 에 더 자세한 내용이 나와 있음","link":"/2019/10/10/YOLO (You Look Only Once)/"},{"title":"Mobilenetv3","text":"Searching for Mobilenet V32019.05.09 Andrew Howard, et. al. from Google AI AbstractMobilenet V3는 hardware-aware NAS (Neural Architecture Search)기법인 NetAdapt을 사용하여 타겟하는 Mobile Device CPU에서 Latency를 줄임과 동시에 성능을 끌어올릴 수 있음을 보였다는데 의의가 있다. 본 논문에서는 Mobilenet V3를 Mobilenet V3-Large와 MobilenetV3-Small 두 가지 버전으로 나누어 Classification, Object Detection과 Semantic Segmentation을 수행하였다. Segmantic Segmentation의 경우는 LR-SPP (Lite Reduced-Spatial Pyramid Pooling)라는 새로운 Decoder구조를 제안하였다. 실험 결과 MobilenetV3-Lage의 Classification은 MobilenetV2에 비해 ImageNet에서 Top-1 accuracy가 3.2% 향상됨과 동시에 Latency가 15% 감소하였다.(MobilenetV3-Small은 accuracy 4.6% 감소) 1. Introduction이 논문은 모바일 디바이스 상에서 속도-성능간 trade-off를 최적화하기 위한 방안들을 다룬다. Complementary Search Engine : Latency &amp; Accuracy를 목표값으로 NAS (Neural Architecture Search)를 진행함. 모바일 셋팅에 최적화된 형태의 비선형 함수 H-swish 함수 사용 새로운 network 구조 제안 : Inverted Resudial + Squeeze-and-Excite 새로운 Segmentation Decoder (LR-SPP) 2. Related WorkAccuracy-Latency 최적화를 위한 다양한 연구 결과를 요약하면 다음과 같다. SqueezeNet : 1x1 conv로 parameter 수를 줄임 Mobilenet V1 : Depthwise Separable conv로 channel-wise 곱셈 paremeter 를 줄임 Mobilenet V2 : Inverted Residual과 Linear Bottleneck을 사용하여 모바일 디바이스 상에서 효율적인 구조 채택 ShuffleNet : Group conv + Channel shuffle을 사용하여 MAC 를 줄임 CondenseNet : Group Conv에 사용될 connections를 학습함 ShiftNet : Shift operation을 point-wise conv와 사용하여 계산량이 많은 Spatial conv 대체 3. Efficient Mobile Building BlocksMnasNet은 Mobilenet V2의 bottleneck 기본 뼈대에서 squeeze-and-excite구조와 light attention module을 추가한 구조를 채택하고 있다. Mobilenet V3는 MnasNet구조에서 swish non-linearity로 업데이트하였으며, fixed-point arithmetic구조에서 비효율적인 sigmoid를 hard sigmoid 구조를 채택하였다. 4. Network SearchNetwork Search는 network의 최적 Block를 찾는데 Platform-aware NAS와 Layer 수 최적화를 위한 NetAdapt 알고리즘을 사용한다. 4.1 Platform-Aware NAS for Block-wise SearchPlatform-Aware NAS는 [43] MnasNet: Platform-Aware Nerual Architecture Search for Mobile 에서 다루고 있는 다중목적 보상함수를 사용하고 있다.$$ACC[m]×[LAT[m]/TAR]^w$$위 식은 Parto-optimal solution을 최적화하기 위해 Target Latency [TAR]항을 분모로 모델 m에 대하여 Accuracy [ACC[m]]와 Latency[LAT[m]] 항을 사용한다. *w은 ACC와 *LAT 간의 비율을 조정하는 항이다. 기존 논문 [43]에서는 w=-0.07를 사용하였지만, 본 논문에서는 작은 모델에 최적화하도록 LAT\\에 weight를 더 주기 위해 w=-0.15를 주어 모델 m을 찾는다. 4.2 NetAdapt for Layer-wise SearchNetAdapt는 *[46] Netadapt: Platform-Aware neural network adaptation for mobile applications*에서 다루고 있다. 요약하면, 4.1절에서 다중목적 보상 NAS로 찾은 seed architecture로 시작한다. 각 step에 대해서 새로운 proposal을 제안한다. 이때, proposal은 이전 step에서의 model에 비해 latency가 최소한계치 δ 만큼 감소시켜야 한다. δ=0.01L L= seed model의 Latency Pretrained weight 값으로 이전 step에서 사용한 model 중 truncated된 부분을 제외한 부분을 채택하며, 새로 생성된 layer는 randomly initialized시킨다. Proposal한 model들을 T step만큼 fine tuning을 거치고 accuracy를 생성한다. T=10,000를 사용 Evaluation Metric을 통해 proposal된 model들 중 최고의 proposal model을 찾는다. Eval Metric =max $$Δ|ACC|\\overΔLAT$$ Insight: Proposal이 discrete하기 때문에 trade-off curve slope를 최대화 함 Proposal 시 변경 가능한 요소들 Expansion Layer 크기 Bottleneck 크기 Target Latency에 도달할 때까지 1~2 과정을 반복한다. 5. Network ImprovementsNetwork Search로 찾은 모델을 더 개선하기 위해 계산량이 많은 Layer를 변경하고, Quantization frinedly한 H-swish 비선형 함수를 제안한다. 5.1 Redesigning Expensive LayersN기존 Network Search로 찾은 모델은 시작, 끝 Layer에서 유독 많은 계산량을 보였다. 이는 기존 Network Search Method의 search space를 벗어나는 부분으로, 수동 tuning을 하여 Accuracy는 불변하고, Latency를 줄여주었다. Final Layer 7×7 Conv Layer into Average pooling Layer Same Accuracy 10 ms Latency (15% of running time) &amp; 30MAdds drop Initial Layer channel 32 channel into 16 channel Non-linearity Function : swish or ReLU Same Accuracy 3 ms Latency &amp; 10MAdds drop 5.2 Nonlinearitiesswish는 *[36] Searching for Activation Function*에서 제안한 비선형 함수로 ReLU를 대체하여 neural network의 accuracy를 향상시킨다. 이때, sigmoid σ(x)를 사용하기에 계산량의 증가가 있다.$$swish(x)=x⋅σ(x)$$sigmoid 함수 σ(x)의 계산량을 모바일 디바이스 상에서 줄이기 위한 두 가지 방법을 제안한다. h-swish $$h−swish[x]=x\\frac{ReLU6(x+3)}6$$ ReLU6는 어떤 Hardware / Software platform에서도 최적화가 가능하다. Framework마다 Sigmoid 구현하는데 따른 numerical precision loss를 없앨 수 있다. Quantized Sigmoid조차도 ReLU보다 느리다. 위 세 가지 이유로, sigmoid 함수를 ReLU6로 바꾼다. h-swish 후 배치 비선형 함수로 인한 Latency는 뒤로 갈 수록 feature-map의 크기가 줄어듦에 따라 감소한다. 따라서, h-swish를 network의 뒤에만 배치하는 것이 더 효과적임을 실험적으로 구했다. 위에 언급한 최적화를 사용함에도 불구하고, h-swish는 여전히 latency가 존재한다. 그럼에도 accuracy-latency tradeoff가 긍정적이고, 향후 software optimize에 따른 성능 향상을 염두하여 Network의 뒷부분에 H-swish 비선형 함수를 사용한다. ReLU와 같은 linear 함수 사용 시, latency bottleneck은 메모리 접근 시 발생함. 효과적으로 메모리 배치 시 최적화 가능함. 5.2.1. Large Squeeze-and-Excite MnasNet 논문에서는 Squeeze-and-excite의 channel은 convolution layer의 채널 수에 상대적이었다. 하지만, 본 논문에서는 Expansion Layer의 1/4 사이즈로 고정시켰다. 이로 인한 accuracy는 향상한 반면, parameter는 소폭만 증가헀다. 5.3 Mobilenet V3 DefinitionMobilenet V3 - Large 구조는 아래와 같다. (Mobilenet-Small은 위 Table 2 참고)* 6. Experiments 비교 대상: Accuracy vs. Latecny &amp; MAdds (Multiply adds) Training setup 학습 환경 : 4x4 TPU Pod Optimizer : RMSPropOptimizer with 0.9 Momentum (Tensorflow) Batch size: 4,096 (128 images per chip) Learning Rate 스케줄 : 0.01 initial rate of decaying 0.01 every 3 step dropout : 0.8 l2 weight decay 1e-5 Exponential moving Average with 0.9999 decay Batch Norm : average decay of 0.99 Latency Measurement Hardware: Google Pixel phones Framework: Tensorflow Lite Benchmark Tool Core 갯수: Single 6.1 Classification Dataset: ImageNet 6.2 Result Mobilenet V3-Large 1.0 성능(Accuracy) &gt; SOTA Mobile Device Target 모델 Mobilenet V3-Small 0.75 지연시간(Latency) &lt; SOTA Mobile Device Target 모델 이미지 해상도(Resolution)이 큰 Mobilenet-V3-Small이 Mobilenet-V3-Large보다 미세하게 성능이 우수함. 하지만, 이미지 해상도는 보통 결정된 값이기에 큰 의미는 없는듯 6.2.1 Ablation Study 전체 Network를 H-swish로 대체하면 성능-속도 trade-off가 좋지 않음 성능 증가 (0.9%) 속도 감소 (-32%) Network 뒷부분만 교체하면 성능-속도 trade-off가 조아짐 성능 증가(0.7%) 속도 감소 (-12%) 크게 의미 없는 수치같음… 6.2 Detection Datset: MsCOCO OD Architecture : SSD Lite (OS 16 ~ OS256) OS 16 (C4) MobilenetV3-Lage 13th bottleneck block MobileentV3-Small 9th bottleneck block OS 32 (C5) After pooling layer (Small &amp; Large) Reduced Channel into Half ImageNet의 클래스 수 (1,000)에 비해 MsCOCO의 클래스 수 (80)이 작기에 기존 channel 수는 redundunt하다고 여김 실제로 성능하락 없이 속도 향상(15%)만 가져옴 Mobilent V2와 동일한 성능 (22.0 mAP vs. 22.1 mAP)이면서 빠른 속도 (150 ms vs. 200 ms) 25% 속도 향상 보임 6.4 Semantic Segmentation Dataset : CityScapes Fine Images (mIOU) Pretraiend Model 사용 x Channel 절반 사용 (OD와 동일한 이유) CityScapes 클래스 19 &lt;&lt; ImageNet 클래스 1,000 LR-ASPP (Lite Reduced design of Atrous Spatial Pyramid Pooling module) 1x1 conv + Global Average Pooling Layer (+ Squeeze-and-Excite) Pooling Layer에 더 큰 stride, kernerl을 사용함 마지막 Layer에 Atrous conv.와 skip connection을 사용함. Mobilenet V3-Large: ESPNet v2, CCC2, ESPNetv1보다 각각 10.5%, 10.6%, 12.3% 성능 향상 + 무게 감소 (Madds) Mobilenet V3-Small: ESPNet v2, CCC2, ESPNetv1보다 각각 1.7, 1.59, 2.24 배 속도 향상 + 6.2% 성능 향상 7. Conclusion Classificaiton, Object Detection, Semantic Segmentation 모두 좋은 결과를 보임 Mobile model의 차세대 Network 제시 비선형 함수 h-swish 사용 양자화에 친화적인 Squeeze-and-Excite 사용 LR-ASPP 선보임.","link":"/2019/07/28/Mobilenetv3/"}],"tags":[{"name":"Static Generator, Hexo, Typora, Blog","slug":"Static-Generator-Hexo-Typora-Blog","link":"/tags/Static-Generator-Hexo-Typora-Blog/"},{"name":"YOLO v2","slug":"YOLO-v2","link":"/tags/YOLO-v2/"},{"name":"YOLO v1","slug":"YOLO-v1","link":"/tags/YOLO-v1/"},{"name":"Mobilent v3, AI, Object Detection, Quantization, Tensorflow Lite","slug":"Mobilent-v3-AI-Object-Detection-Quantization-Tensorflow-Lite","link":"/tags/Mobilent-v3-AI-Object-Detection-Quantization-Tensorflow-Lite/"}],"categories":[]}