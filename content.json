{"pages":[],"posts":[{"title":"Gradient-Harmonized-SSD-Tutorial","text":"Created: Oct 30, 2019 8:05 AMTags: 실습 Gradient Harmonized SSD 실습MMdetection API저자 소개 및 수상 경력 Multimedia Lab. The Chinese University of Hong Kong (CUHK) COCO 2018 Challenge (Instance Segmentation / Object Detection task) 1등 COCO 2019 Challenge (Object Mask) 3등 API 특징 학습 및 테스트하는 툴 제공 https://github.com/open-mmlab/mmdetection 테스트: tools/test.py 학습에 필요한 기능이 모듈별로 세분화 datasets: build_dataset으로 타겟 데이터셋 loader 호출 models: build_detector로 타겟 물체인식 모델 호출 apis init_dist: mpi, pytorch,등 학습 back-end engine 지정 train_detector: 분산학습 유무, logging 유무 지정 get_random_seed: 임의 초기 Seed 값 호출 get_root_logger: working directory 경로, 데이터셋 경로 등 지정 Argument: 학습 환경에 맞는 argument를 쉽게 바꾸도록 함 gpu: GPU 갯수 지정 workdir: 학습된 모델 weight, log 저장소 resume_from: pretrained weight 호출경로 TrainingEnvironment Nvidia GPU P-100 8대 nccl back-end Batch size = 2 Hyerparameter GHM-C Loss M (number of Unit Region) = 30 GHM-R Loss (ASL1 loss) mu : 0.02 Image size: 1333 by 800 (paper just mentioned 800) Optimizer : SGD learning rate: 0.01 → 0.001 (9epoch) → 0.0001 (12epoch) step decay w/ warm-up epoch: 14 Network baseNet: ResNext-101 w/ 64 neck: same as RetinaNet (FPN) bbox head: same as RetinaNet Dataset MsCOCO 2017 train 122k val 5K E.T.C. mmcv error mmcv: pip install -e mmcv Path PYTHONPATH 추가 mmdet error “ImportError: cannot import name ‘deform_conv_cuda” mmdet: python setup.py install → build → develop Result MsCOCO Eval Dataset Epoch: 9 epoch (Same as the paper) Conf. Threshold : 0.05 → 0.1 Max nms_Pre : 1,000 → 500 eval Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.538Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.745Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.585Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.397Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.598Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.664Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.403Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.637Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.667Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.507Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.809 → 논문 대비 성능 향상 (mAP 12%)","link":"/2019/11/02/Gradient-Harmonized-SSD-Tutorial/"},{"title":"Typora-Hexo","text":"Typora-Hexo를 이용하여 1시간 안에 Blog 글 올리기! 안녕하세요. 이제 Blog를 본격적으로 시작한지 일주일 밖에 안된 블룸베리입니다. 블로그를 시작하기로 마음먹고, 블로그에 글을 게제하기까지 우여곡절이 많았는데요, 이 글을 읽는 여러분께서는 조금이나마 시간을 아낄 수 있도록 글을 준비해보았습니다. (시간은 소중하니깐요..ㅎㅎ) 그럼 시작하겠습니다! (참고로 이 글은 https://www.stand-firm-peter.me/2018/01/06/Start/ 글을 바탕으로 제가 느낀점을 추가하여 작성한 글입니다!) 1. 블로그 url 정하기제일 먼저, 블로그를 시작하려면 블로그 주소(url)를 받을 블로그 프레임워크를 정해야 해요. 저는 아래 세 블로그 프레임워크를 두고 고민해보았습니다. Tistory Naver Blog Github 각자의 장단점이 있는데요, 저 같이 Tech Blog를 주로 작성하려고 하는 사람은 Github를 활성화해주면 향후 스펙용, 이직 준비용으로 좋아서 Github로 결정하게 됩니다. Github로 블로그 생성하기 궁금하신 분들은 위 링크를 참고해주세요. 이처럼 자신이 블로그 운영을 하는 목적에 따라 블로그 프레임워크를 정하시면 좋을 것 같습니다. 2. 정적 블로그 생성 툴 정하기WordPress와 같이 PHP를 자유자래로 사용하는 컴퓨터 공학도가 아닌 이상, 정적 생성기를 사용하는 것은 필수라고 본다. 정적 생성기로 무엇을 써도 무방하며, 본인이 사용하기 간편하고, contents를 효율적이고 깔끔하게 만들 수 있는 툴을 사용하면 된다. Jekyll : 시도해보았으나, ruby어로 구현된 문법에 친숙하지 못해 이내 포기함. Hexo : 학교 후배가 사용하여 알게됨. node.js를 사용하며, 시작이 간편하고, 기본으로 제공하는 템플릿 적용이 원활하여 이로 결정하게 됨. Gatsby : 웹 개발에 많이 사용하는 react를 사용한다고 함. (잘모름) 3. Markdown Editor 정하기Github 블로그는 모드 textfile의 일종인 markdown 형식을 사용하여 글을 게제한다. 따라서, 효율적인 Markdown Editor 사용이 블로그 글 게시 활동에 핵심이라 해도 과언이 아니다. 두 가지 editor 중에 hotkey가 직관적이라 사용하기 쉽고, Image Insert 혹은 Math module 첨부가 용이한 typora라는 editor를 최종 선택하였다. 3.1 Typora Image Insert 방법Typora에 drag-and-drop으로 Image Insert하기 위해서는 몇 가지 typora setting을 건드려야 한다. 상대 경로 활성화 하기 좌측 상단에 ‘File-Preference-Images Insert 항목에서 ‘Use relative path if possible‘ 체크 박스를 활성화 한다. Copy Image file to folder 활성화 하기 이게 뭐하는거냐? Github에 사진을 업로드 하기 위해서는 반드시 서버 상에도 그 이미지를 업로드 해야 한다. 다시 말해, 단순히 typora editor에 drag-and-drop한다고 해서 markdown 파일에 그림이 첨부되는게 아니다. 좀 복작하지만, 비슷한 효과(?)를 내기 위해 drag-and-drop했을 경우 설정된 경로로 drop한 이미지가 복사되는 기능을 활성화 한 것이라 볼 수 있다. 방법은 아래와 같다. ‘Edit - Image Tools - When Inesrt Local Image - Copy Image File to Folder’ 체크 ​ 저장 경로 설정하기 제일 중요한 부분이다! 이것 때문에 주말에 5시간동안 삽질을 했다.. 부디 그런일 마시길. 저장 경로를 이해하기 위해서 먼저 Hexo의 _config.yml파일을 살펴봐야 한다. Github서버에 Markdown파일을 업로드하면 _config.yml파일에 permalink에서 지정한 형식으로 경로가 설정된다. 예를 들어 ‘Mobilenetv3.md’파일을 ‘나의 directory/source/Mobilenetv3.md ‘라는 곳에 생성했다고 하자. 실제로 deploy를 하게되면 github Web 주소 상에서는 https://BloomBerry.github.io/2019/07/28/Mobilenetv3 라는 주소로 index.html파일이 생성된다. 이때 중요한 것은 동일한 경로, 예컨데 ‘나의 directory/source’에 Mobilenetv3라는 디렉토리를 만들고 그곳에 이미지를 넣는다고 생각하면 오산이다. Hexo는 themes 디렉토리 내부 경로만 참조하기 때문에 ‘themes/자신의 themes’ 경로에 이미지를 추가해줘야 한다. 예컨데 icarus를 사용한다면 ‘themes/icaurs/source/MobilenetV3’ 경로에 image를 추가해야 한다. 이를 원활하게 하기 위해 typora-root-url과 typora-copy-images-to를 이용하면 drag-and-drop하여 Image를 추가할 수 있다. 주의할 점은 github에 deploy하기 전에 반드시 두 줄을 #으로 주석처리해야 한다. [Optional] 글 별로 이미지를 관리하고 싶을 때 개인의 취향에 맞기겠지만, markdown별로 첨부한 이미지를 관리하고 싶을 때는 _config.yml에 post_asset_folder를 True로 만들어 주면 된다. command 창에 hexo new New_Title.md라고 치면 typora-copy-images-to에서 설정한 경로에 md파일명과 동일한 디렉토리가 생성되는 것을 볼 수 있다. 짜잔! 이 글역시 위 방법으로 만든 글이다. 이 글을 작성하는데 1시간 정도밖에 걸리지 않았다. 놀랍군!!","link":"/2019/07/30/Typora-Hexo/"},{"title":"darknet으로 Yolo V3 학습 Tutorial","text":"darknet으로 Yolo V3 학습 TutorialCreated: Oct 17, 2019 6:26 AMTags: 실습 이번 튜토리얼은 darknet framework을 사용하여 Yolo-v3 학습시키는 방법을 다룬다. 1. YOLO V3 상세 — Feature 추출기: 기존에 Yolo-v2에서는 Darknet-19 (19층의 CNN Layer로 구성된 baseNet)을 사용 Yolo-v3로 넘어오면서 Darknet-53을 사용함 연속된 e 3 × 3 and 1 × 1 CNN 와 ResNet에 사용된 shortcut connections 사용 자세한 논문은 여기 참고 2. Yolo v3 학습 in Darknet &amp; Pascal VOC1. Darknet 설치git clone https://github.com/pjreddie/darknet cd darknet make GPU(CUDA)를 사용하려면, Makefile에서 GPU, CUDNN = 1, OPENCV = 0으로 설정 후 make 2. 데이터셋 준비Pascal VOC Dataset Downloadwget https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar wget https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar wget https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar tar xf VOCtrainval_11-May-2012.tar tar xf VOCtrainval_06-Nov-2007.tar tar xf VOCtest_06-Nov-2007.tarText file 생성 darknet은 각 이미지마다 같은 경로에 같은 이름으로 txt파일이 필요함 ~/JPEGImages/00001.png ~/JPEGImages/00002.png ... ~/labels/00001.txt ~/labels/00002.txt ... txt 내용: 11 0.34419263456090654 0.611 0.4164305949008499 0.262 14 0.509915014164306 0.51 0.9745042492917847 0.972 : [0 ~ class-1] = / , = / , = / , = / x, y는 박스의 중심점을 의미 (center-form ) Example My Image Size: 360 * 480 it have one object ie:dog image_width = 360 image_height = 480 absolute_x = 30 (dog x position from image) absolute_y = 40 (dog y position from image) absolute_height = 200 (original height of dog from image) absolute_width = 200 (original width of dog from image)The final Text file have. It have five args and all five args between 0 to 1 &lt;class_number&gt; (&lt;absolute_x&gt; / &lt;image_width&gt;) (&lt;absolute_y&gt; / &lt;image_height&gt;) (&lt;absolute_width&gt; / &lt;image_width&gt;) (&lt;absolute_height&gt; / &lt;image_height&gt;) ie:0 (30/360) (40/480) (200/360) (200/480)0 0.0833 0.0833 0.556 0.417 Pascal은 위의 작업을 해주는 코드 실행하면 자동으로 txt 생성됨 wget https://pjreddie.com/media/files/voc_label.py python voc_label.py 3. TrainingImageNet pretrained weight 다운로드darknet53 모델의 사전학습된 weight 다운로드 here (76 MB). Preparing YOLOv3 configuration filesYOLOv3는 3가지 파일이 있어야 학습이 가능함 YOLOv3 needs certain specific files to know how and what to train. We’ll becreating these three files(.data, .names and .cfg) and also explain theyolov3.cfg and yolov3-tiny.cfg. cfg/voc.data class 갯수, trai n/validation dataset 경로 정보가 있는 텍스트 파일 주소, class 정보 (string-to-int mapping), backup (weight file 저장 경로) data/voc.names 클래스 이름 (순서 중요) cfg/yolov3.cfg Network 학습에 필요한 hyper-parameter, Network 구조 정의된 파일 GPU ≥ 2이면 yolov3.cfg 사용. 그 이하이면 tiny-yolo.cfg 사용(GPU 2GB 미만) Step 1: (tiny-yolo.cfg 인 경우) i) yolov3-tiny.cfg를 복사해서 yolo3-tiny-voc.cfg 로 저장 /YoloExample/darknet-v3/cfg$ cp yolov3-tiny.cfg yolov3-tiny-voc.cfg아래 변수를 바꿔준다. Line 3: set batch=64, 매 step마다 64개 이미지를 학습한다는 의미 Line 4: set subdivisions=16, batch는 16만큼 나뉘어져 GPU VRAM 요구사항을 만족시키며 학습. Line 127: set filters=(classes + 5)*3 여기선 filters=75 Line 135: set classes=20, 학습할 클래스 갯수 Line 171: set filters=(classes + 5)*3 여기선 filters=75 Line 177: set classes=20, 학습할 클래스 갯수 Start the Trianing with follow the step 1: train.txt파일을 루트 경로에 복사한 후 아래 명령어 실행 ~/YoloExample/darknet-v3$./darknet detector train cfg/voc.data cfg/yolov3-tiny.cfg darknet53.conv.74OUTPUT: Step 2: (yolov3.cfg 일 경우) i)yolov3.cfg 복사해서 yolov3-voc.cfg 생성 :~/YoloExample/darknet-v3$ cd ~/YoloExample/darknet-v3/cfg$ cp yolov3.cfg yolov3-voc.cfg아래 변수를 바꿔준다. Line 3: set batch=24, this means we will be using 24 images for every training step Line 4: set subdivisions=8, the batch will be divided by 8 to decrease GPU VRAM requirements. Line 603: set filters=(classes + 5)*3 in our case filters=75 Line 610: set classes=20, the number of categories we want to detect Line 689: set filters=(classes + 5)*3 in our case filters=75 Line 696: set classes=2, the number of categories we want to detect Line 776: set filters=(classes + 5)*3 in our case filters=75 Line 783: set classes=20, the number of categories we want to detect Start the Trinaing with follow the step 2: train.txt파일을 루트 경로에 복사한 후 아래 명령어 실행 ~/YoloExample/darknet-v3$./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74학습 시 주의사항:Weightss는 900 step이전에는 100 step마다 저장되지만, 그 이후엔 10,000 step당 저장됨. 필요시 link 참고. Reference https://medium.com/@manivannan_data/how-to-train-yolov3-to-detect-custom-objects-ccbcafeb13d2 http://blog.naver.com/PostView.nhn?blogId=epfam126&amp;logNo=221444115072&amp;categoryNo=14&amp;parentCategoryNo=0&amp;viewDate=&amp;currentPage=1&amp;postListTopCurrentPage=1&amp;from=search https://pjreddie.com/darknet/yolo/ https://medium.com/@manivannan_data/how-to-train-yolov2-to-detect-custom-objects-9010df784f36","link":"/2019/10/19/darknet-Yolo-V3-Tutorial/"},{"title":"2020 야구 암송대회 진동센서 사용법","text":"2020 야구 암송대회 진동센서 사용법Created: Feb 19, 2020 12:04 AM 1. 목적본 문서의 목적은 빛과진리교회에서 매년 진행하는 야구 성경암송대회에 사용될 “야구공 타격감지 시스템”을 구축하고, 운영하는 방법에 관한 메뉴얼을 제공하는데 있다. 문서는 다음i과 같이 구성된다. 시스템 구성도 센서모듈 통신모듈 운영모듈 2. 시스템 구성도1. 센서모듈a. 압전 센서 피에조 진동감지센서모듈/Piezo Vibration/압전세라믹/아두이노 실제로는 센서부와 모듈부의 접촉이 취약하여 진동센서로 대체함 판매처: http://www.eduino.kr/product/detail.html?product_no=295&amp;cate_no=27&amp;display_group=1 ![](/images/2020 야구 암송대회 진동센서 사용법/Untitled.png) 스펙 전원: 3.3V or 5V Signal : Analog Input ![](/images/2020 야구 암송대회 진동센서 사용법/Untitled%201.png) 회로도 참고 사이트: http://www.eduino.kr/product/detail.html?product_no=295&amp;cate_no=27&amp;display_group=1 ![](/images/2020 야구 암송대회 진동센서 사용법/Untitled%202.png) 실습 영상 https://www.youtube.com/watch?v=T-9QObiSrKw&amp;feature=youtu.be 실습 코드 123456789101112131415161718192021222324252627282930313233343536int ledPin = 12; // LED 출력 단자int led2Pin = 7; // LED 출력 단자int piezoPin = A1; // 압전센서 의 아날로그 단자int worth = 0; // 현재 측정 된 값int ledStatus = LOW; // LED 현재 상태 (OFF)(충격 변화)int threshold = 100;int threshold2 = 500; // (중요) 자주 인식 되지 않는 경우 0과 1023 사이의 값을 조정하여 인식한다.void setup() {pinMode(ledPin, OUTPUT); // 핀 12번 LED 출력핀 설정pinMode(led2Pin, OUTPUT); // 핀 7번 LED2 출력핀 설정Serial.begin(9600); // PC로 전송}void loop() { // 셋팅후 연속적 실행worth = analogRead(piezoPin); // A1 핀 값을 판독if (worth &gt;= threshold) { // 임계 값 초과시 이벤트 발생ledStatus = !ledStatus; // LED 상태 (1/0 =시작/종료)digitalWrite(ledPin, ledStatus); // 핀 12번 으로 실행if (worth &gt;= threshold2)ledStatus = !ledStatus; // LED 상태 (1/0 =시작/종료)digitalWrite(led2Pin, ledStatus); // 핀 7번 으로 실행Serial.println(worth); // PC로 값 보내기delay(200);}delay(10); // 시리얼 모니터 프린팅 시간} b. 진동센서![](/images/2020 야구 암송대회 진동센서 사용법/Untitled%203.png) 충격센서/진동센서 모듈 (SW-18010P) 센서가 부착된 위치의 진동을 감지하는 센서 감도조절부를 조절하며 진동의 감도를 원하는 크기로 조절 가능 DO핀은 미사용 기본 1023~1022값을 출력하며, 진동을 하게되면 감소하게되는 센서 진동 판단 기준치 : 1,000 (실제 사용한 기본값) site: http://www.eduino.kr/product/detail.html?product_no=289&amp;cate_no=27 작동 원리 ![](/images/2020 야구 암송대회 진동센서 사용법/_2020-02-25_21-44-06.png) 회로도 ![](/images/2020 야구 암송대회 진동센서 사용법/_2020-02-25_21-48-26.png) 예제 코드 int LED = 9; // LED의 핀 번호 void setup(){ Serial.begin(9600); // 시리얼 통신, 속도는 9600 } void loop(){ int val = analogRead(A0); // 충격 센서의 데이터 받아오기 if (val &lt; 1000){ Serial.print(&quot;Vibration Detected: &quot;); Serial.println(val); } else{ Serial.print(&quot;No Vibration: &quot;); Serial.println(val); } delay(300); } 2. 통신모듈 아두이노 이더넷 Ethernet W5100 micro SD소켓 쉴드 위 통신모듈은 이더넷 통신을 지원함 (서버-클라이언트) Server : 아두이노+w5100 Client : PC상에서 IP주소를 explorer에 입력하여 Client로 서버에 연결 필요한 기기: 라우터 (공유기), 아두이노 우노 필요한 정보: 라우터 맥 주소, IP 작동 원리: 아두이노+w5100이 서버가 되어 공유기에 이더넷으로 연결시키면, 공유기가 서버의 고유 IP주소를 자동으로 생성시켜줌. 이 공유기에 연결된 PC는 서버와 이더넷 통신이 가능하며, (Local Ethernet 통신) 서버의 IP주소로 explorer를 활용하여 접근 하면 Client가 되어 서버의 정보를 받아올 수 있음. ![](/images/2020 야구 암송대회 진동센서 사용법/_2020-02-25_22-13-24.png) 판매처: http://shopping.interpark.com/product/productInfo.do?prdNo=4922988714&amp;gclid=CjwKCAiA1rPyBRAREiwA1UIy8I-_vCNpD-NdJEn_sf_oYi4r3qvmCi8Fqj3roZMTUHbCRbNFhML8XRoCsIQQAvD_BwE ![](/images/2020 야구 암송대회 진동센서 사용법/_2020-02-19_21-44-36.png) 코드 서버 IP주소 Echo 예제 아래 코드로부터 서버의 IP주소를 알 수 있다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;SPI.h&gt;#include &lt;Ethernet.h&gt;// Enter a MAC address for your controller below.// Newer Ethernet shields have a MAC address printed on a sticker on the shieldbyte mac[] = {0x74,0x69,0x69,0x2D,0x30,0x5};// Initialize the Ethernet client library// with the IP address and port of the server// that you want to connect to (port 80 is default for HTTP):EthernetClient client;void setup() {// Open serial communications and wait for port to open:Serial.begin(9600);// this check is only needed on the Leonardo:while (!Serial) {; // wait for serial port to connect. Needed for native USB port only}// start the Ethernet connection:if (Ethernet.begin(mac) == 0) {Serial.println(&quot;Failed to configure Ethernet using DHCP&quot;);// no point in carrying on, so do nothing forevermore:for (;;);}// print your local IP address:printIPAddress();}void loop() {switch (Ethernet.maintain()){case 1://renewed failSerial.println(&quot;Error: renewed fail&quot;);break;case 2://renewed successSerial.println(&quot;Renewed success&quot;);//print your local IP address:printIPAddress();break;case 3://rebind failSerial.println(&quot;Error: rebind fail&quot;);break;case 4://rebind successSerial.println(&quot;Rebind success&quot;);//print your local IP address:printIPAddress();break;default://nothing happenedbreak;}}void printIPAddress(){Serial.print(&quot;My IP address: &quot;);for (byte thisByte = 0; thisByte &lt; 4; thisByte++) {// print the value of each byte of the IP address:Serial.print(Ethernet.localIP()[thisByte], DEC);Serial.print(&quot;.&quot;);}Serial.println();} 진동센서 띄우는 서버 예제 int VibrationPin = A1; int led2Pin = 7; int threshold2 = 1000; #include &lt;Wire.h&gt; #include &lt;SPI.h&gt; #include &lt;Ethernet.h&gt; byte mac[] = { 0xBC,0x96,0x80,0xA3,0xB2,0xF3}; //IPAddress ip(192,168,219,100); IPAddress ip(192,168,219,31); // IP 주소는 “ XXX,XXX,XXX,XXX “ ‘,’ 주의해주세요. // 이더넷 라이브러리 초기화 // 사용할 IP 주소 와 포트 입력 // (‘port 80’ 은 HTTP 의 기본 값 입니다.): EthernetServer server(80); void client(); void setup() { // Open serial communications and wait for port to open: Serial.begin(9600); while (!Serial) { ; // 포트 연결까지 기다리기. 레오나르도 에만 필요합니다. } // 이더넷 서버 연결 시작: Ethernet.begin(mac, ip); server.begin(); Serial.print(“server is at “); Serial.println(Ethernet.localIP()); } void loop() { EthernetClient client = server.available(); if (client) { Serial.println(&quot;new client&quot;); // an http request ends with a blank line boolean currentLineIsBlank = true; while (client.connected()) { if (client.available()) { char c = client.read(); Serial.write(c); // if you’ve gotten to the end of the line (received a newline // character) and the line is blank, the http request has ended, // so you can send a reply if (c == ‘n’ &amp;&amp; currentLineIsBlank) { // send a standard http response header client.println(“HTTP/1.1 200 OK”); client.println(“Content-Type: text/html”); client.println(“Connnection: close”); client.println(); //client.println(““); //client.println(““); //// 브라우저 5초마다 새로고침 //client.println(“&lt;meta http-equiv=&quot;refresh&quot; content=&quot;5&quot;&gt;”); //client.println(““); //client.print(“Welcome EDUINO”); //client.println(““); int worth = 0; // Vibration sensor 값 출력 worth = analogRead(VibrationPin); client.write(worth); Serial.print(worth); //client.println(““); //client.println(““); //client.print(“EDUINO Office”); //client.println(““); //client.println(““); //client.print(“Sensor Output Requested”); //client.println(““); //client.println(““); //client.print(“Pressure : “); client.println(worth); //client.print(“0“); //client.print(“C”); //client.println(““); //client.println(““); //client.println(““); //client.println(““); break; } if (c == ‘n’) { currentLineIsBlank = true; } else if (c != ‘r’) { currentLineIsBlank = false; } } Serial.print(“6”); } //브라우저 데이터 받는 시간 delay(1); //연결 종료: client.stop(); Serial.println(“client disonnected”); } } ​ 3. 운영모듈운영모듈이라 함은, 원하는 목적지(야구대회 진행하는 Client측)에 진동 센서로 받은 데이터를 통신모듈을 거쳐 전달하는 모듈을 말한다. 운영 모듈은 다음과 같이 구성된다. 데이터 파싱부 데이터 파싱부는, 서버로부터 원하는 정보 (센서정보)를 받아오는 부분을 말한다. 이더넷 통신으로 Client가 되어 서버로부터 정보를 받아온다. 예제코드 - 진동이 발생한 경우 ( Sensor Input &lt; Threshold)에만 서버로부터 정보를 요청하는 코드 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144 #include &lt;Wire.h&gt; #include &lt;SPI.h&gt; #include &lt;Ethernet.h&gt; byte mac[] = { 0xBC,0x96,0x80,0xA3,0xB2,0xF3}; //IPAddress ip(192,168,219,100); IPAddress ip(192,168,219,31); // IP 주소는 &quot; XXX,XXX,XXX,XXX &quot; &apos;,&apos; 주의해주세요. // 이더넷 라이브러리 초기화 // 사용할 IP 주소 와 포트 입력 // (&apos;port 80&apos; 은 HTTP 의 기본 값 입니다.): EthernetServer server(80); void client(); void setup() { // Open serial communications and wait for port to open: Serial.begin(9600); while (!Serial) { ; // 포트 연결까지 기다리기. 레오나르도 에만 필요합니다. } // 이더넷 서버 연결 시작: Ethernet.begin(mac, ip); server.begin(); Serial.print(&quot;server is at &quot;); Serial.println(Ethernet.localIP()); } void loop() { int val = analogRead(A0); // 충격 센서의 데이터 받아오기 if (val &lt; 1000){ Serial.print(&quot;Vibration Detected: &quot;); Serial.println(val); } else{ Serial.print(&quot;No Vibration: &quot;); // Piezo sensor 값 출력 Serial.println(val); } delay(100); //진동센서 노이즈 제거하는 기능 //Serial.print(&quot;1&quot;); EthernetClient client = server.available(); //Serial.print(&quot;2&quot;); if (client) { // Serial.print(&quot;3&quot;); //Serial.println(&quot;new client&quot;); // an http request ends with a blank line boolean currentLineIsBlank = true; while (client.connected()) { // Serial.print(&quot;4&quot;); if (client.available()) { // Serial.print(&quot;5&quot;); char c = client.read(); Serial.write(c); // if you&apos;ve gotten to the end of the line (received a newline // character) and the line is blank, the http request has ended, // so you can send a reply if (c == &apos;n&apos; &amp;&amp; currentLineIsBlank) { // send a standard http response header client.println(&quot;HTTP/1.1 200 OK&quot;); client.println(&quot;Content-Type: text/html&quot;); client.println(&quot;Connnection: close&quot;); client.println(); if (val &lt; 1000){ // client.write(val); client.println(val); // Serial.println(val); } else{ client.println(val); // Serial.println(val); } break; } if (c == &apos;n&apos;) { currentLineIsBlank = true; } else if (c != &apos;r&apos;) { currentLineIsBlank = false; } } //Serial.print(&quot;6&quot;); } //브라우저 데이터 받는 시간 delay(1); //연결 종료: client.stop(); Serial.println(&quot;client disonnected&quot;); } }- 데이터 송신부 - Web socket 통신으로 야구대회 진행진에게 정보를 전달하는 서버를 띄우는 역할을 한다. - Web socket 통신이므로 쌍방향 통신이 가능하다. (만, 여기선 일방향 통신만 사용함) - 예제코드 (파이썬) - 디버깅 목적으로 Client 코드도 사용함 - Web Socket 서버부 - 참고 사이트: [https://websockets.readthedocs.io/en/stable/intro.html](https://websockets.readthedocs.io/en/stable/intro.html) #!/usr/bin/env python # WS server example import socket import asyncio import websockets import requests import time URL = &apos;http://192.168.219.31:80&apos; async def hello(websocket, path): print(&quot;try to request: &quot;, URL) while True: response = requests.get(URL) vibration = response.text if response.status_code == 200: # print(&quot;response text: &quot;, response.text) if int(vibration) &lt; 1000: print(&quot;Vibration Accured: &quot;, vibration) await websocket.send(&quot;1&quot;) # time.sleep(0.2) else: print(&quot;No Vibration: &quot;, vibration) else: print(&quot;failed&quot;) print(response) #start_server = websockets.serve(hello, &quot;192.168.219.609&quot;, 8765) #start_server = websockets.serve(hello, &quot;192.168.219.182&quot;, 8765) start_server = websockets.serve(hello, &quot;192.168.219.140&quot;, 8765) #print(start_server.keys()) asyncio.get_event_loop().run_until_complete(start_server) asyncio.get_event_loop().run_forever()- Web Socket Client - 참고 사이트: [https://websockets.readthedocs.io/en/stable/intro.html](https://websockets.readthedocs.io/en/stable/intro.html) #!/usr/bin/env python # WS client example import asyncio import websockets async def hello(): #uri = &quot;ws://localhost:8765&quot; #uri = &quot;ws://192.168.219.182:8765&quot; uri = &quot;ws://192.168.219.140:8765&quot; async with websockets.connect(uri) as websocket: # name = &quot;dd&quot; # await websocket.send(name) greeting = await websocket.recv() print(f&quot;&lt; &quot;, greeting) asyncio.get_event_loop().run_until_complete(hello()) ``` ​ ​ 시연 영상 /images/2020 야구 암송대회 진동센서 사용법/__200224.mp4 4. 향후 계획 실제 야구공-막대지지대에 부착된 상태로 진동센서의 민감도를 조정하여, 오검출/미검출 사례 예방 렌선을 10m짜리로 확보하여 야구공-막대지지대 근처에 아두이노를 설치화고, 운영모듈과 연결될 수 있도록 조치함. 예비 연습","link":"/2020/02/25/2020 야구 암송대회 진동센서 사용법/"},{"title":"SSD","text":"Single Shot Multibox Detector Created: Oct 17, 2019 12:23 PM Tags: 논문 0. Background ‘15.12 Archive 등재 Wei Liu, et. al Abstract Aspect Ratio, Scales를 feature map마다 별개로 둔 앵커 박스를 생성 다양한 크기로 출현하는 물체를 인식하기 위함 Single Neural Network의 출력은 앵커박스로 이산화 Prediction 시, 물체의 클래스 probability score와 앵커 박스로부터 조정될 크기 값을 출력 Input: 300x300) Pascal VOC 207 test : 74.3 mAP // 59 FPS@TitanX (Faster RCNN: 73.2mAP // 7 FPS, YOLO: 63.4mAP) 1. IntroductionContribution Single Shot Detector (YOLO) 보다 정확하고, slower techiques (Faster R-CNN)보다 빠른 실시간 물체인식 알고리즘 제안 고정된 Anchor-Box 기준으로 offset과 score를 추출하기 위해 Feature map에 Convolution Layer를 추가한 구조 서로 독립적인 Feature map에서 scale, aspect ratio를 바꿔서 정확도 향상시킴 Pascal VOC, COCO, ILSVRC 비교하여 성능&amp;속도 trade-off 향상됨을 입증함 2. The Single Shot Detector (SSD) 4x4 &amp; 8x8 Feature Map에서 생성된 anchor-box 예시. 각각 빨간색, 파란색 anchor-box는 G.T.인 개, 고양이와 matching되어 positive anchorbox, 나머진 nregative anchorbox로 (배경으로) 학습이 됨 목적함수(Loss)는 confidance loss + localization loss의 합계 2-1. Model 전반부 (BaseNet): 이미지로부터 높은 해상도의 feature map 추출 VGG 16 사용 후반부 (Detection Head): 물체 인식 출력 값 생성 다양한 크기의 Feature-map: Output Stride 8, 16, 32, 64, 128, 256 사용 Convolution Predictor를 검출기로 활용함 m x n 크기의 Feature-map이 채널 크기가 p일 경우 3 x 3 x p 개의 커널로 생성함 Grid 당 k anchor box일 경우, 출력 갯수는 kmn(c+4) 2-2. Training 매칭 전략 Highest jaccard overlap이 되는 앵커박스를 GT와 매칭 Threshold 0.5이상을 추가로 매칭 Recall을 높이기 위해(?) 목표함수 x는 matched index 0이면 unmatched, 1은 matched. N은 matching된 앵커박스 갯수 c는 class probability 위치 손실 L1손실함수. 추정 박스(l)과 GT박스(g)간의 차이를 손실값으로 봄 앵커박스의 중심(dcx,dcy)와의 offset을 학습하도록 구현됨 너비(dwi), 높이(dhi)로 정규화시킴 모두 matching된 앵커박스만 손실값으로 봄 분류 손실 매칭된 앵커박스(positive)와 매칭 안된 앵커박스(negative)의 비율을 1:3 으로 학습 더 안정적으로 학습됨 Feature map의 grid에서 생성된 박스는 반드시 receptive field가 아님. Scale값을 조정해서 앵커박스의 크기를 조절할 수 있음 m개의 feature map일 때, linear scaling rule 적용하여 scale 산출 다양한 위치, 크기의 물체를 견고히 인식하기 위해 data augmentation기법 적용 Horizontal flip, Random patch, image distortion, etc Basenet은 vgg16 사용 3. Experimental Results3-1. Pascal VOC2007 Pascal voc 2012 test 결과 : 81.6mAP COCO pretrained+512x512+voc12 trainval 학습셋으로 사용 3-2. Model analysis Data Augmentation에 의한 성능 향상이 앞도적으로 높았음 (8.8% mAP 향상) Anchor Box의 모양을 다양화한게 성능 향상에 도움이 됨 Atrous Conv을 사용함으로써 성능은 유지하되, 속도 향상이 있었음 (20% 향상) 흰색은 True Positive, 파란색(Local error) 빨간색(Class error)은 True Negative 보라색은 False Positive 빨간 실선은 (IOU Threshold=0.5) Recall, 점선은 (IOU Threshold=0.1) Recall Localization error가 작음. 위치 regression잘 한다는 뜻 Classification error가 큼. 검출기 head를 localization과 공유하기 때문 작은 물체를 더 못 검출. 작은 물체를 검출하는 layer가 한참후에 detection head로 들어오므로, 정보손실 발생하기 때문. 3-3. Pascal VOC2012 COCO로 pretrained시킴 VOC07 train/val/test + VOC12 train/val 데이터를 학습용으로 사용 Faster RCNN보다 성능 높음 3-4. COCO Pascal VOC와 비슷한 양상임. 작은 물체는 큰 물체에 비해 상대적으로 성능 향상이 적었음 3-5. Preliminary ILSVRC results ILSVRC 2014 DET 데이터로 학습함 43.4mAP 3-6. Data Augmentation for Small Object Accuracy Random Crop은 Zoom In 효과를 줌 Zoom Out 효과를 주기 위해 기존 이미지를 16배 키운 사이즈로 중간값으로 나머지 부분 채운후 학습. 2-3 mAP 향상 있었음 3-7. Inference Time Inference 시간을 효율적으로 하기 위해 Confidence Threshold 0.01 이하 Anchor Box는 걸름 그 이후 NMS를 IOU Threshold 0.5로 주고 Max Output Box 갯수를 200으로 함 1.7ms 소요 (새로 추가한 Layer들은 2.4ms 소요) 4. Related Work Selective Search Image(R-CNN) 혹은 feature map(Fast-RCNN, MultiBox)을 crop한 후, Fixed size로 reshape하는 작업이 필요함. 여러개의 잠재 Bounding Box에 대해 별도의 Classifier, Regressor를 둠으로써 속도 저하 Region Proposal Network Faster-RCNN에서 Anchor Box와 함께 처음 사용됨. CNN의 출력 feature map을 RPN의 입력으로 사용하여 CNN을 통과시켜 자동으로 Region Propose함. 뒤에 Classifier, Regressor를 별도로 둠으로써 두 단계를 거침. Classifier, Regressor없이 바로 Classification하면 SSD와 같음. One-shot Detector SSD에서 Anchor Box를 최말단 Feature map에서 1개의 Anchor Box 사용 → Overfeat SSD에서 Anchor Box를 최말단 Feature map에서 grid당 1개의 Anchor Box 사용 → YOLO 5. Conclusion Multiple feature map에서 multi-scale Convolutional BBox 사용함 SSD512는 Faster RCNN보다 3배 빠르면서 성능이 VOC, MSCOCO, ILSVRC 모두 좋았음 SSD300은 59 FPS로 실시간 물체인식 알고리즘임 6. Reference https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e","link":"/2019/10/19/papers/SSD/"},{"title":"Tesla의 auto-pilot beta 버전 논문 리뷰","text":"Pseudo-LiDAR from Visual Depth Estimation:Bridging the Gap in 3D Object Detection for Autonomous Driving keyword: 3D object detection, autonomous driving, LiDAR, image-based depth estimation, depth estimation, pseudo-LiDAR representations, KITTI 3D object detection leaderboard, stereo-image-based, 3D point cloud code : https://github.com/mileyan/pseudo_lidar archive : https://arxiv.org/pdf/1812.07179.pdf Image based 3D object detection의 한계KITTI benchmark 기준.. Image based SOTA = 10% AP LiDAR only SOTA = 66% Image + LiDAR = 73% Image based 성능이 부진한 이유 (기존) Stereo depth-estimator는 depth에 대해 quadratic한 error를 갖는다. LiDAR는 depth에 대해 linear한 error를 갖는다. Image based 성능이 부진한 이유 (제안) ConvNet 기반의 3D 물체 인식 시스템에서 표현 방식의 잘못된 선택 (poor choice of representations of the 3D information for ConvNet-based 3D object detection system) LiDAR signal representation : 3D point clouds or BEV 관점 Image based representation : 2D image 멀리 떨어진 물체는 매우 작기에 검출이 어려움 인접한 pixel group은 3차원 공간을 함께 가리킴 논문이 제안하는 2 step 검증방식 Step 1. Monocular / Stereo Image 기반 depth estimation → 3D point cloud로 표현방식 변환 So called “Pseudo-LiDAR” BaseLine [10] DORN : multi-scale feature를 기존의 회기방법과 함께 사용 [3] PSMNet : Siamese network를 3D ConV와 사용 Step 2. 기존의 LiDAR-based 3D object Detection pipeline 사용 [17] AVOD (Aggregated ~) : BEV 영역으로 3D space 표현 x axis : width y axis : depth channel : height [25] frustrum PointNet : 3D point cloud로 3D space 표현 결과 Metrics : KITTI benchmark IoU@0.7 “moderately hard” car class 45.3% 3D AP on validation set (기존 Image-based SOTA 대비 350% 상승) LiDAR-based System과 Image-based 간의 격차를 절반으로 줄임 Depth Estimation Stereo camera기반 disparity estimation algorithm $$D(u,v) = \\frac{f_U\\times b}{Y(u,v)}$$ $f_U$ : focal length for left camera $Y(u,v)$ : horizontal disparity $b$ : baseline $u,v$ : camera pixel coordinate Pseudo-LiDAR generation![](Untitled 1.png) $$(depth) z=D(u,v) \\ (width) x=\\frac{(u-c_U)\\times z}{f_U} \\ (height) y=\\frac{(v-c_V)\\times z}{f_V}$$ $c_U, c_V$ camera center pixel location $f_V$ : vertical focal length 3D object detection 3D point cloud 기반의 Network 중 SOTA인 PSMNet을 baseline으로 사용 Bird’s Eye View기반의 Network 중 SOTA인 AVOD를 baseline으로 사용 Data representation matters2D Depth Map에 2D Conv를 적용하면 실제 domain으로 정사영 시, 꺠지는 현상 발생함 ![](Screenshot from 2020-11-06 22-16-41.png) 4. Experiments Dataset: Kitti object detection Train : 3,712 Val : 3,769 Test : 7,518 Metric : 3D &amp; BEV AP @0.5:0.7 for Car class Image기반의 baseline Mono3D : Monocular Camera기반 3DOP : Stereo Camera기반 MLF : Mono / Stereo기반 RGB 3 channel에 Depth 정보를 concatnate해서 Faster-RCNN 기반으로 학습 Stereo disparity estimation Learning based PSMNet : KITTI Stereo 2015 200 dataset으로 학습된 weight는 사용 불가 사유 : KITTI validation set에 일부 data가 겹침. → KITTI 3K trainset으로 fineTuned w/ pseudo disparity GT PSMNet* 성능 : KITTI stereo 2015, 1.86% error DISPNet : MLF Stereo도 DISPNet 기반. DISPNet-C : with correlation DISPNet-S: w/o correlation 성능 : KITTI stereo 2015, 4.32% error Non-learning based SPS-Stereo Monocular depth estimation [10] DORN를 사용. Pseudo-LiDAR generation From Image coordinate to Velodyne LiDAR’s coordinate system Using calibration matrices provided. 3D object Detection Image + Lidar based Frustum PointNet-v1 [25] AVOD-FPN [17] Experimental Results![](Untitled 2.png) 파란 볼드체 (pseudo-LiDAR)기반 이 image based SOTA baseline 보다 월등히 좋은 성적을 냄 Impact of Image Representation![](Untitled 3.png) MLF는 DISPNET을 사용하기에 같은 Disparity 성능을 냄. 단지 Representation을 Frontal (2D image)에서 Pseudo-LiDAR를 사용했을 경우 성능이 훨씬 나아짐 Impact of stereo disparity estimation accuracy![](Untitled 4.png) PSMNET이 DISPNET보다 stereo disparity estimation accuracy가 좋으나 (1.86% &gt; 4.32% error) Depth estimation 성능은 Detection algorithm에 큰 영향을 안끼침 (Compare F-POINTNET DISPNET-C vs. F-POINTNET PSMNET) 왜 그럴까? Disparity accuracy가 Depth accuracy를 대변하지 않을 수 있음 voxel 기반으로 point cloud를 다룰 경우 (F-POINTNET), depth noise에 둔감할 수 있음 Comparison to LiDAR informationTable 1 참고. 물체가 멀리 있을 수록, IOU threshold가 높을수록 (0.7 &gt; 0.5) 성능이 하락함. Depth accuracy 영향임. 0.4 MPixel → 고해상도로 변경 시 해결 가능 Pedestrian and cyclist detection짐![](Untitled 5.png) Car보다 작고, hard하기에 성능이 LiDAR대비 떨어짐 Image based에서는 논외임. 좋은 starting point Results on the test set![](Untitled 6.png) Validation set에 Overfitting된게 아님을 확인 Visualization (Validation set)![](Screenshot from 2020-11-06 22-16-41-1604670677961.png) Supplementary Material Ground plane estimation AVOD 의 경우 ground plane을 아래와 같이 4개의 parameter로 표현함 $$H, w_x , w_y, w_z$$ Pseudo-LiDAR point cloud 정보를 RANSAC을 이용해 아래 식을 만족하는 ground plane estimation 수행 $${w^Tp}+h=0$$ Where $W$ : ground plane norm vector (w_y=-1) $p$: Pseudo-LiDAR 3D point cloud $h$ : height 위 식에 적용되는 Point cloud 점들은 camera에 근접한 것들만 사용 Height: 1.5m ~ 1.86m Depth : 0 ~ 40m Width : -15m ~ 15m Pseudo Disparity Estimation Equation 1을 적용하여 Pseudo LiDAR point → Pseudo Disparity 추출 여러 cloud point가 disparity캡에 적용 되면 랜덤하게 선택함 선택되지 않은 point는 disparity없는 상태로 학습 SPS-Stereo Non-learning 기반의 Stereo disparity 접근법 KITTI Stereo 2015 성능 : 3.84%","link":"/2020/11/06/papers/Tesla의 auto-pilot beta 버전 논문 리뷰/"},{"title":"YOLO v1","text":"YOLO (You Look Only Once)0. Background‘15.06 Archive ‘16.05 CVPR Joseph Redmon, et. al. Abstract 물체 인식 문제를 클래스 정보를 포함한 앵커 박스를 공간적으로 분리된 관점에서 회기 문제로 봄. Single Network 관점에서 end-to-end로 학습하여 속도가 빠르고 최적화가 가능함. (Titan X: 45 FPS) Fast Yolo는 155 FPS SOTA인 R-CNN에 비해 localization 에러는 많으나, False Positive가 적다. 당시 SOTA였던 DPM, R-CNN에 비해 일반화 효과가 뛰어남. 1. Introduction Deformable Parts Model (DPM)은 Sliding Window를 이용하여 모든 image에 대해 균일한 stride로 움직인다. (비효율적) RCNN은 Region Proposal을 통과한 후, potential Bounding Box에 대해 Classifier를 통과시키고, post-processing을 통과시킨다. 각각의 파트가 따로 학습되어, 학습이 용이하지 않고, 최적화가 어렵다. YOLO은 end-to-end로 한 번에 이미지 pixel domain에서 Box coordinate를 출력시킨다. 이미지 Resize –&gt; Single CNN 통과 –&gt; Confidence 값을 기준치로 filtering 다른 OD에 비해 장점 학습이 용이하다. [Figure 1] 물체 인식 문제 –&gt; 회귀 문제로 봤음. (Simple Pipeline) Titan X 기준 45 FPS –&gt; 1 이미지 처리당 &lt; 25 [ms] latency –&gt; real time Global Context를 encode 함 –&gt; Background False Positive 줄어듦. DPM이나 R-CNN은 지엽적인 정보만 봄 –&gt; YOLO에 비해 Background에 대한 FP가 2배 이상 나타남 일반화 효과가 뛰어남 자연의 이미지에서 학습하고 Artwork으로 test하면, 다른 OD Network에 비해 좋은 성능이 나옴. 새로운 Domain Data나 기대하지 않은 image에 더 좋은 성능이 날 것임 2. Unified Detection R-CNN에서 3부분이 담당하던 기능을 Single DNN으로 일원화함 전체 이미지에서 처리한 feature를 사용하여 bounding box를 예측함 Real-time end-to-end Training (학습의 용이성) High Precision (당시 real-time Detector보다 2배 이상 성능) S x S grid를 기준으로 물체와의 중심점의 차이값 ($\\Delta x, \\Delta y$), 전체 이미지에 대한 물체의 크기 ($w, h$) , 그리고 물체인 정도를 G.T. box와 grid의 IOU를 가지고 판단함. $$Pr(Class|Object) * Pr(Object) * IOU_{pred}^{truth} =Pr(Class)*IOU_{pred}^{thres}$$ 각 Grid 마다 B개의 Bounding Box를 생성함. (B=2 for YOLO v1) Objectness Score는 GT와 Predicted Box간의 IOU를 가지고 판단함. ( if Objectness &gt; thres ? true : false ) Probability는 B와 무관하게 Grid마다 1개씩만 추측한다. 최종 Output은 $$S \\times S\\times(B*5+C)$$이다. Pascal VOC에서 C=20, B=2, S=7을 사용함. 2-1. Network Design 앞 단은 Conv Layer + 뒷 단은 FC Layer로 구성 YOLO는 24개의 Conv Layer, Fast YOLO는 9개의 Conv Layer로 구성 GoogleNet를 참조하여 Network를 Design. (단, Inception모듈 대신 Simple하게 3x3 Conv 뒤에 1x1 Conv를 둠) 2-2. Training ImageNet Pretrained Weight 사용 ( Top 1 accuracy: 88%) Input Image : 224 x 224 –&gt; 448 x 448로 바꿈 ($\\because$Image resolution이 높아야 precision이 향상) $w, h$는 Image의 width, height로 정규화 ($\\because$0~1 사이로 두기 위해) $x, y$는 grid cell과의 offset으로 paramitarized Leaky Relu 사용 Loss Function $|_i$ 는 i번째 grid cell을 의미 i번째 grid cell에 물체가 있는 경우만 classification penalty부여 $|_{ij}^{obj}$ 는 i번째 grid cell의 j번째 bounding box를 의미 i번째 grid cell j번째 bounding box가 물체가 있는 경우 localization penalty 부여 $\\lambda_{coord}=5, \\lambda_{noobj}=0.5$ 사용 localization error에 가중치를 줌으로써 noobj에 따른 gradient decent=0수렴 현상 방지 2-3. Inference 98개의 Box만 생성 크기가 큰 물체, grid cell의 경계면에 맞닿아 있는 물체의 경우 여러개의 grid로부터 물체검출이 됨. Non-Maximum Suppression을 통해 약 2~3% mAP 향상 가능 2-4. Limitations of YOLO Grid cell당 2개의 물체, 1개의 클래스만 검출가능하기 때문에, 작고 군집된 물체를 잡는데 약함. 물체의 모양을 데이터 기반으로 학습하다 보니, Aspect Ratio (가로세로 비율), 물체의 형태가 새로운 물체의 경우 일반화 효과가 떨어짐. Feature가 coarse하다 보니, 물체의 Localized resolution이 좋지 못함. 큰 물체, 작은 물체 모두 동일하게 작은 에러에 대한 loss를 부여함. 이 때문에 local error 발생 (IOU를 가지고 loss를 잡는게 더 좋을 것) 3. Comparison to Other Detection Systems Detection System은 모두 Robust한 Feature를 추출하고, Localizer 혹은 Classifier를 feature space를 통과시킨다. Sliding Window로 전체 Feature를 훑거나, 특정 region만 본다. Deformable Parts Model (DPM): Feature 추출, 영역 분류, 높은 score의 영역으로부터 BBox 검출 등의 단계를 disjoint하게 실행함 R-CNN: Selective Search를 이용하여 잠재적인 BBox 2,000개를 생성함. SVM은 박스 score를 내고, NMS 통과시킴 Other Fast Detectors: Selective Search를 Region Proposal로 대체함. Computation 공유 Deep MultiBox: DNN을 이용한 Single Object Detection에 사용됨. ROI를 추론함. Class prediction (probability X) OverFeat: Sliding Window를 사용하여 DNN으로 Localization을 수행. Global context 사용 못하는 한계 MultiGrasp: Grid 방식 도입. DNN으로 Single Object 검출에 사용. Class 추출 X, Multi Object X 4. Experiments Pascal VOC 2007, 2012를 Fast-RCNN과 비교하며 실험 진행 4-1. Comparison to Other Real-Time Systems mAP &amp; speed Trade-off를 살펴봄 Fastest DPM: Selective Search를 static BBox proposal로 대체함 Real-time(30FPS) 만족하는 Detector중에서 최고성능임 Faster R-CNN은 YOLO에 비해 10 mAP가 좋으나, 실시간성이 보장이 안됨. 4-2. VOC 2007 Error Analysis Fast R-CNN vs. YOLO in Pascal VOC 2007 Yolo가 Localization error가 많았음 Fast R-CNN이 YOLO에 비해 False Positive가 약 3배 높음 Correct: correct class &amp; IOU &gt; 0.5 Localization: correct class &amp; 0.1 &lt; IOU &lt; 0.5 Similar : class is similar &amp; IOU &gt; 0.1 Other: class is wrong &amp; IOU &gt; 0.1 Background: IOU &lt; 0.1 for any object 4-3. Combining Fast R-CNN and YOLO YOLO와 Fast-RCNN의 Ensemble 결과, Fast-RCNN은 약 2~3% mAP 향상이 있었음 이는 BaseNet만 바꾼 Fast-RCNN간의 Ensemble보다 월등히 향상된 결과임 ($\\because$YOLO는 Fast-RCNN과 다른 error 양상을 보이기 때문임) 4-4. VOC 2012 Results Fast-RCNN+YOLO조합은 ‘15.11월 당시 Top-4 달성함 4-5. Generalizability: Person Detection in Artwork Fast-RCNN은 AP가 많이 낮아짐($\\because$Selective search로 local context만 봄) 이에 반해 YOLO는 AP가 적게 낮아짐 ($\\because$global context를 봄) 5. Real-Time Detection In The Wild 6. Conclusion 실시간을 보장하는 가장 빠른 일반화된 물체 인식 알고리즘임 http://pjreddie.com/yolo/ 에 더 자세한 내용이 나와 있음","link":"/2019/10/10/papers/YOLO (You Look Only Once)/"},{"title":"YOLO v2","text":"YOLO v20. Background Archive: ‘16.12 CVPR 2017 Joesph Redmon Abstract SOTA training 기법 적용 (Multi-scale training) Fast YOLO v2: 67 FPS, 76.8 mAP (YOLO v1: 45 FPS, 63.4 mAP) YOLO v2: 40FPS, 78.6 mAP &gt;&gt; Better than SSD, FR-RCNN ImageNet 200개 Class 중 44개의 class label만 가지고 19.3mAP –&gt; Unlabelled class (156개) 1. Introduction 일반화 가능한 물체 인식 알고리즘의 조건 : 빠르고, 정확하며, 수 많은 물체를 분류 가능해야 함. Classfication Label 비용 &lt;&lt; Detection Label 비용 –&gt; 현실적으로 수 많은 물체에 대한 label data 확보 어려움 새로운 학습 기법 제안 : Joint Training + Dataset Combination Classifjication label을 계층적으로 물체를 분류함 기존에 수 많은 Classfication Data와 Object Detection Data를 함께 사용함 Classification Data는 vocabulary를 늘리고 robustness를 높여주기 위함. 2. Better Recall과 localization error가 여전히 RPN계열에 비해 낮음. 이 두 가지에 집중하여 네트워크 개발함 Batch norm: 다른 Regularization 방법 (ex. Drop Out)을 없애도 됨. mAP 약 2% 상승 High Resolution Classifier: ImageNet Pretrain 시, 이미지 해상도를 2배 증가 (224 –&gt; 448)시켜 10에폭 돌림. mAP 약 4% 상승 Grid cell의 OD Head를 FC Layer에서 Convolution Layer로 변경함 물체의 위치 좌표를 직접 찾지 않고, Anchor Box간의 offset을 구함 (FR-CNN 따라함) 448 –&gt; 416으로 변경함. ($\\because$ 홀수로 두어, 중심점이 생기도록 하기 위함) 최종 Feature Map 크기는 13x13 Anchor Box 갯수 98 에서 &gt;1,000 로 많아짐 Anchor Box를 사용 : mAP (69.5 –&gt; 69.2) / Recall (81% –&gt; 88%) Anchor Box Size : K-mean clustering 이용함 K값에 따라 계산량 vs 성능 trade-off가 있음. 기존에 FR-CNN의 Hand-picked Anchor Box보다 우수함. Dimension Prediction: SSD, RPN계열이 쓰는 Offset을 추정하는 방식 ($\\Delta x, \\Delta y$) 과 달리, grid cell내로 제한된 영역에서 grid cell과의 상대적 위치 x, y값을 직접 추정함. Offset 방식: 초기에 불확실성이 있음 ($\\because $이미지 내 어디든 존재 가능) Grid cell방식: Anchor Box의 중심점을 grid cell별로 제한함으로 학습이 용이해짐. 성능이 약 5% mAP 상승 Fine Grained Features: 13x13 Feature Map에 26x26 Feature Map을 Concat하여 박스를 추정함. 1% mAP 상승 개인적으론 Feature Map의 Local한 정보를 살리지 못하는 방식이라고 생각됨. (특허 회피 같은 느낌?) Multi-Scale Training: 매 10 에폭마다 {320, 352, .., 608}의 이미지 사이즈 중 랜덤으로 입력 이미지 크기를 resize하여 학습. 효과: 다양한 해상도의 이미지로 학습함 –&gt; 이미지 해상도에 Robust?? 288x288의 경우 90 FPS 608x80x의 경우 mAP &gt; FR-RCNN mAP Further Experiments: Pascal VOC12, COCO eval 결과 모두 SOTA 동등 이상 성능 COCO의 경우, 물체의 크기의 분산이 큼 –&gt; K-clustering 효과 떨어짐으로 SSD에 비해 성능 저하 되는 것으로 사료? 3. Faster Darknet-19: Yolo v2에서 사용한 base network. (5.58 BOPs, 72.9%/91.2% Top-1/5 Accuracy) VGG-16: (30.69 BOPs, 90% Top-5 Accuracy) 3x3 Conv + 1x1 Conv, BN, Global Avg Pool, Channel x2 After Pooling, etc Classification Network 학습 : 224x224로 학습 후, 448x448로 10 에폭 학습 Detection Network 학습 : Classification Network에서 Final Conv Layer만 제거 후, 3개의 3x3 Conv w/ 1,024 Channel Layer 추가 + 1x1 Conv Layer 추가 4. Stronger Detection과 Classification 전용 Dataset을 Jointly 학습하는 기법 제안함. Classification 레이블 데이터일 경우, Classification 관련 구조만 back-prop. 학습 Detection 레이블 데이터일 경우, 전체 구조 back-prop. 학습 Multi Label 기법 사용 ($\\because$Detection Label, Classification Label 상호 베타적이지 않음. ex. ‘Dog’ ‘요크셔테리어’) 계층적 구조를 띈 ‘WordTree’ 라벨링 제안 $Pr(Norfolk terrier) = Pr(Norfolk terrier|terrier) * Pr(terrier | hunting dog) … Pr(mammal | animal) * Pr(animal | object)$ Classification 데이터에서 $Pr(Object)=1$ Multi Labelling Detection 데이터는 YOLO v2의 Objectness score를 활용. Depth Search Method로 최대 score &amp; threshold 넘는 layer까지 레이블을 취함 COCO + Imagenet Combination WordTree를 이용하여 mapping함 ImageNet의 9,000 class + MS COCO 80 class 데이터 모두 사용함 MS COCO 데이터를 over-sampling하여 비율을 4:1로 맞춰 학습함. Anchor prior는 3개만 사용 (output의 사이즈를 고려함) ImageNet Detection Task: (200개 class 중 44개만 학습에 사용됨.) 19.7 mAP (156개 에 대해서는 15.6 mAP) Weakly-supervised 방식임 여전히 실시간 보장함 동물 종류는 잘 구분하나, ‘cloth’, ‘sunglasses’ 등 옷 종류는 잘 구분 못함. 옷 종류가 구분하기 어려워 그런 것으로 사료됨 5. Conclusion Detection &amp; Classification 데이터를 jointly 활용하는 Weakly-supervised 학습기법 제안 비단 Detection Task뿐 아니라, Semantic Segmentation Task에도 활용할 예정 ImageNet + COCO 데이터셋을 이용해 실시간, 범용 물체인식 알고리즘 YOLO V2 제안 Multi-scale Training 등 여러 학습기법 제안","link":"/2019/10/10/papers/YOLO v2/"},{"title":"Mobilenetv3","text":"Searching for Mobilenet V32019.05.09 Andrew Howard, et. al. from Google AI AbstractMobilenet V3는 hardware-aware NAS (Neural Architecture Search)기법인 NetAdapt을 사용하여 타겟하는 Mobile Device CPU에서 Latency를 줄임과 동시에 성능을 끌어올릴 수 있음을 보였다는데 의의가 있다. 본 논문에서는 Mobilenet V3를 Mobilenet V3-Large와 MobilenetV3-Small 두 가지 버전으로 나누어 Classification, Object Detection과 Semantic Segmentation을 수행하였다. Segmantic Segmentation의 경우는 LR-SPP (Lite Reduced-Spatial Pyramid Pooling)라는 새로운 Decoder구조를 제안하였다. 실험 결과 MobilenetV3-Lage의 Classification은 MobilenetV2에 비해 ImageNet에서 Top-1 accuracy가 3.2% 향상됨과 동시에 Latency가 15% 감소하였다.(MobilenetV3-Small은 accuracy 4.6% 감소) 1. Introduction이 논문은 모바일 디바이스 상에서 속도-성능간 trade-off를 최적화하기 위한 방안들을 다룬다. Complementary Search Engine : Latency &amp; Accuracy를 목표값으로 NAS (Neural Architecture Search)를 진행함. 모바일 셋팅에 최적화된 형태의 비선형 함수 H-swish 함수 사용 새로운 network 구조 제안 : Inverted Resudial + Squeeze-and-Excite 새로운 Segmentation Decoder (LR-SPP) 2. Related WorkAccuracy-Latency 최적화를 위한 다양한 연구 결과를 요약하면 다음과 같다. SqueezeNet : 1x1 conv로 parameter 수를 줄임 Mobilenet V1 : Depthwise Separable conv로 channel-wise 곱셈 paremeter 를 줄임 Mobilenet V2 : Inverted Residual과 Linear Bottleneck을 사용하여 모바일 디바이스 상에서 효율적인 구조 채택 ShuffleNet : Group conv + Channel shuffle을 사용하여 MAC 를 줄임 CondenseNet : Group Conv에 사용될 connections를 학습함 ShiftNet : Shift operation을 point-wise conv와 사용하여 계산량이 많은 Spatial conv 대체 3. Efficient Mobile Building BlocksMnasNet은 Mobilenet V2의 bottleneck 기본 뼈대에서 squeeze-and-excite구조와 light attention module을 추가한 구조를 채택하고 있다. Mobilenet V3는 MnasNet구조에서 swish non-linearity로 업데이트하였으며, fixed-point arithmetic구조에서 비효율적인 sigmoid를 hard sigmoid 구조를 채택하였다. 4. Network SearchNetwork Search는 network의 최적 Block를 찾는데 Platform-aware NAS와 Layer 수 최적화를 위한 NetAdapt 알고리즘을 사용한다. 4.1 Platform-Aware NAS for Block-wise SearchPlatform-Aware NAS는 [43] MnasNet: Platform-Aware Nerual Architecture Search for Mobile 에서 다루고 있는 다중목적 보상함수를 사용하고 있다.$$ACC[m]×[LAT[m]/TAR]^w$$위 식은 Parto-optimal solution을 최적화하기 위해 Target Latency [TAR]항을 분모로 모델 m에 대하여 Accuracy [ACC[m]]와 Latency[LAT[m]] 항을 사용한다. *w은 ACC와 *LAT 간의 비율을 조정하는 항이다. 기존 논문 [43]에서는 w=-0.07를 사용하였지만, 본 논문에서는 작은 모델에 최적화하도록 LAT\\에 weight를 더 주기 위해 w=-0.15를 주어 모델 m을 찾는다. 4.2 NetAdapt for Layer-wise SearchNetAdapt는 *[46] Netadapt: Platform-Aware neural network adaptation for mobile applications*에서 다루고 있다. 요약하면, 4.1절에서 다중목적 보상 NAS로 찾은 seed architecture로 시작한다. 각 step에 대해서 새로운 proposal을 제안한다. 이때, proposal은 이전 step에서의 model에 비해 latency가 최소한계치 δ 만큼 감소시켜야 한다. δ=0.01L L= seed model의 Latency Pretrained weight 값으로 이전 step에서 사용한 model 중 truncated된 부분을 제외한 부분을 채택하며, 새로 생성된 layer는 randomly initialized시킨다. Proposal한 model들을 T step만큼 fine tuning을 거치고 accuracy를 생성한다. T=10,000를 사용 Evaluation Metric을 통해 proposal된 model들 중 최고의 proposal model을 찾는다. Eval Metric =max $$Δ|ACC|\\overΔLAT$$ Insight: Proposal이 discrete하기 때문에 trade-off curve slope를 최대화 함 Proposal 시 변경 가능한 요소들 Expansion Layer 크기 Bottleneck 크기 Target Latency에 도달할 때까지 1~2 과정을 반복한다. 5. Network ImprovementsNetwork Search로 찾은 모델을 더 개선하기 위해 계산량이 많은 Layer를 변경하고, Quantization frinedly한 H-swish 비선형 함수를 제안한다. 5.1 Redesigning Expensive LayersN기존 Network Search로 찾은 모델은 시작, 끝 Layer에서 유독 많은 계산량을 보였다. 이는 기존 Network Search Method의 search space를 벗어나는 부분으로, 수동 tuning을 하여 Accuracy는 불변하고, Latency를 줄여주었다. Final Layer 7×7 Conv Layer into Average pooling Layer Same Accuracy 10 ms Latency (15% of running time) &amp; 30MAdds drop Initial Layer channel 32 channel into 16 channel Non-linearity Function : swish or ReLU Same Accuracy 3 ms Latency &amp; 10MAdds drop 5.2 Nonlinearitiesswish는 *[36] Searching for Activation Function*에서 제안한 비선형 함수로 ReLU를 대체하여 neural network의 accuracy를 향상시킨다. 이때, sigmoid σ(x)를 사용하기에 계산량의 증가가 있다.$$swish(x)=x⋅σ(x)$$sigmoid 함수 σ(x)의 계산량을 모바일 디바이스 상에서 줄이기 위한 두 가지 방법을 제안한다. h-swish $$h−swish[x]=x\\frac{ReLU6(x+3)}6$$ ReLU6는 어떤 Hardware / Software platform에서도 최적화가 가능하다. Framework마다 Sigmoid 구현하는데 따른 numerical precision loss를 없앨 수 있다. Quantized Sigmoid조차도 ReLU보다 느리다. 위 세 가지 이유로, sigmoid 함수를 ReLU6로 바꾼다. h-swish 후 배치 비선형 함수로 인한 Latency는 뒤로 갈 수록 feature-map의 크기가 줄어듦에 따라 감소한다. 따라서, h-swish를 network의 뒤에만 배치하는 것이 더 효과적임을 실험적으로 구했다. 위에 언급한 최적화를 사용함에도 불구하고, h-swish는 여전히 latency가 존재한다. 그럼에도 accuracy-latency tradeoff가 긍정적이고, 향후 software optimize에 따른 성능 향상을 염두하여 Network의 뒷부분에 H-swish 비선형 함수를 사용한다. ReLU와 같은 linear 함수 사용 시, latency bottleneck은 메모리 접근 시 발생함. 효과적으로 메모리 배치 시 최적화 가능함. 5.2.1. Large Squeeze-and-Excite MnasNet 논문에서는 Squeeze-and-excite의 channel은 convolution layer의 채널 수에 상대적이었다. 하지만, 본 논문에서는 Expansion Layer의 1/4 사이즈로 고정시켰다. 이로 인한 accuracy는 향상한 반면, parameter는 소폭만 증가헀다. 5.3 Mobilenet V3 DefinitionMobilenet V3 - Large 구조는 아래와 같다. (Mobilenet-Small은 위 Table 2 참고)* 6. Experiments 비교 대상: Accuracy vs. Latecny &amp; MAdds (Multiply adds) Training setup 학습 환경 : 4x4 TPU Pod Optimizer : RMSPropOptimizer with 0.9 Momentum (Tensorflow) Batch size: 4,096 (128 images per chip) Learning Rate 스케줄 : 0.01 initial rate of decaying 0.01 every 3 step dropout : 0.8 l2 weight decay 1e-5 Exponential moving Average with 0.9999 decay Batch Norm : average decay of 0.99 Latency Measurement Hardware: Google Pixel phones Framework: Tensorflow Lite Benchmark Tool Core 갯수: Single 6.1 Classification Dataset: ImageNet 6.2 Result Mobilenet V3-Large 1.0 성능(Accuracy) &gt; SOTA Mobile Device Target 모델 Mobilenet V3-Small 0.75 지연시간(Latency) &lt; SOTA Mobile Device Target 모델 이미지 해상도(Resolution)이 큰 Mobilenet-V3-Small이 Mobilenet-V3-Large보다 미세하게 성능이 우수함. 하지만, 이미지 해상도는 보통 결정된 값이기에 큰 의미는 없는듯 6.2.1 Ablation Study 전체 Network를 H-swish로 대체하면 성능-속도 trade-off가 좋지 않음 성능 증가 (0.9%) 속도 감소 (-32%) Network 뒷부분만 교체하면 성능-속도 trade-off가 조아짐 성능 증가(0.7%) 속도 감소 (-12%) 크게 의미 없는 수치같음… 6.2 Detection Datset: MsCOCO OD Architecture : SSD Lite (OS 16 ~ OS256) OS 16 (C4) MobilenetV3-Lage 13th bottleneck block MobileentV3-Small 9th bottleneck block OS 32 (C5) After pooling layer (Small &amp; Large) Reduced Channel into Half ImageNet의 클래스 수 (1,000)에 비해 MsCOCO의 클래스 수 (80)이 작기에 기존 channel 수는 redundunt하다고 여김 실제로 성능하락 없이 속도 향상(15%)만 가져옴 Mobilent V2와 동일한 성능 (22.0 mAP vs. 22.1 mAP)이면서 빠른 속도 (150 ms vs. 200 ms) 25% 속도 향상 보임 6.4 Semantic Segmentation Dataset : CityScapes Fine Images (mIOU) Pretraiend Model 사용 x Channel 절반 사용 (OD와 동일한 이유) CityScapes 클래스 19 &lt;&lt; ImageNet 클래스 1,000 LR-ASPP (Lite Reduced design of Atrous Spatial Pyramid Pooling module) 1x1 conv + Global Average Pooling Layer (+ Squeeze-and-Excite) Pooling Layer에 더 큰 stride, kernerl을 사용함 마지막 Layer에 Atrous conv.와 skip connection을 사용함. Mobilenet V3-Large: ESPNet v2, CCC2, ESPNetv1보다 각각 10.5%, 10.6%, 12.3% 성능 향상 + 무게 감소 (Madds) Mobilenet V3-Small: ESPNet v2, CCC2, ESPNetv1보다 각각 1.7, 1.59, 2.24 배 속도 향상 + 6.2% 성능 향상 7. Conclusion Classificaiton, Object Detection, Semantic Segmentation 모두 좋은 결과를 보임 Mobile model의 차세대 Network 제시 비선형 함수 h-swish 사용 양자화에 친화적인 Squeeze-and-Excite 사용 LR-ASPP 선보임.","link":"/2019/07/28/papers/Mobilenetv3/"}],"tags":[{"name":"darknet, tutorial, Gradient-Harmonized-SSD Tutorial","slug":"darknet-tutorial-Gradient-Harmonized-SSD-Tutorial","link":"/tags/darknet-tutorial-Gradient-Harmonized-SSD-Tutorial/"},{"name":"Static Generator, Hexo, Typora, Blog","slug":"Static-Generator-Hexo-Typora-Blog","link":"/tags/Static-Generator-Hexo-Typora-Blog/"},{"name":"darknet, tutorial, Yolo V3","slug":"darknet-tutorial-Yolo-V3","link":"/tags/darknet-tutorial-Yolo-V3/"},{"name":"Arduino, Ethernet Communicaiton, Arduino, Vibration Sensor, Web socket communication","slug":"Arduino-Ethernet-Communicaiton-Arduino-Vibration-Sensor-Web-socket-communication","link":"/tags/Arduino-Ethernet-Communicaiton-Arduino-Vibration-Sensor-Web-socket-communication/"},{"name":"SSD","slug":"SSD","link":"/tags/SSD/"},{"name":"pseudo-lidar","slug":"pseudo-lidar","link":"/tags/pseudo-lidar/"},{"name":"YOLO v1","slug":"YOLO-v1","link":"/tags/YOLO-v1/"},{"name":"YOLO v2","slug":"YOLO-v2","link":"/tags/YOLO-v2/"},{"name":"Mobilent v3, AI, Object Detection, Quantization, Tensorflow Lite","slug":"Mobilent-v3-AI-Object-Detection-Quantization-Tensorflow-Lite","link":"/tags/Mobilent-v3-AI-Object-Detection-Quantization-Tensorflow-Lite/"}],"categories":[]}