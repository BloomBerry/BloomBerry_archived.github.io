{"pages":[],"posts":[{"title":"Mobilenetv3","text":"Searching for Mobilenet V32019.05.09 Andrew Howard, et. al. from Google AI AbstractMobilenet V3는 hardware-aware NAS (Neural Architecture Search)기법인 NetAdapt을 사용하여 타겟하는 Mobile Device CPU에서 Latency를 줄임과 동시에 성능을 끌어올릴 수 있음을 보였다는데 의의가 있다. 본 논문에서는 Mobilenet V3를 Mobilenet V3-Large와 MobilenetV3-Small 두 가지 버전으로 나누어 Classification, Object Detection과 Semantic Segmentation을 수행하였다. Segmantic Segmentation의 경우는 LR-SPP (Lite Reduced-Spatial Pyramid Pooling)라는 새로운 Decoder구조를 제안하였다. 실험 결과 MobilenetV3-Lage의 Classification은 MobilenetV2에 비해 ImageNet에서 Top-1 accuracy가 3.2% 향상됨과 동시에 Latency가 15% 감소하였다.(MobilenetV3-Small은 accuracy 4.6% 감소) 1. Introduction이 논문은 모바일 디바이스 상에서 속도-성능간 trade-off를 최적화하기 위한 방안들을 다룬다. Complementary Search Engine : Latency &amp; Accuracy를 목표값으로 NAS (Neural Architecture Search)를 진행함. 모바일 셋팅에 최적화된 형태의 비선형 함수 H-swish 함수 사용 새로운 network 구조 제안 : Inverted Resudial + Squeeze-and-Excite 새로운 Segmentation Decoder (LR-SPP) 2. Related WorkAccuracy-Latency 최적화를 위한 다양한 연구 결과를 요약하면 다음과 같다. SqueezeNet : 1x1 conv로 parameter 수를 줄임 Mobilenet V1 : Depthwise Separable conv로 channel-wise 곱셈 paremeter 를 줄임 Mobilenet V2 : Inverted Residual과 Linear Bottleneck을 사용하여 모바일 디바이스 상에서 효율적인 구조 채택 ShuffleNet : Group conv + Channel shuffle을 사용하여 MAC 를 줄임 CondenseNet : Group Conv에 사용될 connections를 학습함 ShiftNet : Shift operation을 point-wise conv와 사용하여 계산량이 많은 Spatial conv 대체 3. Efficient Mobile Building BlocksMnasNet은 Mobilenet V2의 bottleneck 기본 뼈대에서 squeeze-and-excite구조와 light attention module을 추가한 구조를 채택하고 있다. Mobilenet V3는 MnasNet구조에서 swish non-linearity로 업데이트하였으며, fixed-point arithmetic구조에서 비효율적인 sigmoid를 hard sigmoid 구조를 채택하였다. 4. Network SearchNetwork Search는 network의 최적 Block를 찾는데 Platform-aware NAS와 Layer 수 최적화를 위한 NetAdapt 알고리즘을 사용한다. 4.1 Platform-Aware NAS for Block-wise SearchPlatform-Aware NAS는 [43] MnasNet: Platform-Aware Nerual Architecture Search for Mobile 에서 다루고 있는 다중목적 보상함수를 사용하고 있다.$$ACC[m]×[LAT[m]/TAR]^w$$위 식은 Parto-optimal solution을 최적화하기 위해 Target Latency [TAR]항을 분모로 모델 m에 대하여 Accuracy [ACC[m]]와 Latency[LAT[m]] 항을 사용한다. *w은 ACC와 *LAT 간의 비율을 조정하는 항이다. 기존 논문 [43]에서는 w=-0.07를 사용하였지만, 본 논문에서는 작은 모델에 최적화하도록 LAT\\에 weight를 더 주기 위해 w=-0.15를 주어 모델 m을 찾는다. 4.2 NetAdapt for Layer-wise SearchNetAdapt는 *[46] Netadapt: Platform-Aware neural network adaptation for mobile applications*에서 다루고 있다. 요약하면, 4.1절에서 다중목적 보상 NAS로 찾은 seed architecture로 시작한다. 각 step에 대해서 새로운 proposal을 제안한다. 이때, proposal은 이전 step에서의 model에 비해 latency가 최소한계치 δ 만큼 감소시켜야 한다. δ=0.01L L= seed model의 Latency Pretrained weight 값으로 이전 step에서 사용한 model 중 truncated된 부분을 제외한 부분을 채택하며, 새로 생성된 layer는 randomly initialized시킨다. Proposal한 model들을 T step만큼 fine tuning을 거치고 accuracy를 생성한다. T=10,000를 사용 Evaluation Metric을 통해 proposal된 model들 중 최고의 proposal model을 찾는다. Eval Metric =max $$Δ|ACC|\\overΔLAT$$ Insight: Proposal이 discrete하기 때문에 trade-off curve slope를 최대화 함 Proposal 시 변경 가능한 요소들 Expansion Layer 크기 Bottleneck 크기 Target Latency에 도달할 때까지 1~2 과정을 반복한다. 5. Network ImprovementsNetwork Search로 찾은 모델을 더 개선하기 위해 계산량이 많은 Layer를 변경하고, Quantization frinedly한 H-swish 비선형 함수를 제안한다. 5.1 Redesigning Expensive LayersN기존 Network Search로 찾은 모델은 시작, 끝 Layer에서 유독 많은 계산량을 보였다. 이는 기존 Network Search Method의 search space를 벗어나는 부분으로, 수동 tuning을 하여 Accuracy는 불변하고, Latency를 줄여주었다. Final Layer 7×7 Conv Layer into Average pooling Layer Same Accuracy 10 ms Latency (15% of running time) &amp; 30MAdds drop Initial Layer channel 32 channel into 16 channel Non-linearity Function : swish or ReLU Same Accuracy 3 ms Latency &amp; 10MAdds drop 5.2 Nonlinearitiesswish는 *[36] Searching for Activation Function*에서 제안한 비선형 함수로 ReLU를 대체하여 neural network의 accuracy를 향상시킨다. 이때, sigmoid σ(x)를 사용하기에 계산량의 증가가 있다.$$swish(x)=x⋅σ(x)$$sigmoid 함수 σ(x)의 계산량을 모바일 디바이스 상에서 줄이기 위한 두 가지 방법을 제안한다. h-swish $$h−swish[x]=x\\frac{ReLU6(x+3)}6$$ ReLU6는 어떤 Hardware / Software platform에서도 최적화가 가능하다. Framework마다 Sigmoid 구현하는데 따른 numerical precision loss를 없앨 수 있다. Quantized Sigmoid조차도 ReLU보다 느리다. 위 세 가지 이유로, sigmoid 함수를 ReLU6로 바꾼다. h-swish 후 배치 비선형 함수로 인한 Latency는 뒤로 갈 수록 feature-map의 크기가 줄어듦에 따라 감소한다. 따라서, h-swish를 network의 뒤에만 배치하는 것이 더 효과적임을 실험적으로 구했다. 위에 언급한 최적화를 사용함에도 불구하고, h-swish는 여전히 latency가 존재한다. 그럼에도 accuracy-latency tradeoff가 긍정적이고, 향후 software optimize에 따른 성능 향상을 염두하여 Network의 뒷부분에 H-swish 비선형 함수를 사용한다. ReLU와 같은 linear 함수 사용 시, latency bottleneck은 메모리 접근 시 발생함. 효과적으로 메모리 배치 시 최적화 가능함. 5.2.1. Large Squeeze-and-Excite MnasNet 논문에서는 Squeeze-and-excite의 channel은 convolution layer의 채널 수에 상대적이었다. 하지만, 본 논문에서는 Expansion Layer의 1/4 사이즈로 고정시켰다. 이로 인한 accuracy는 향상한 반면, parameter는 소폭만 증가헀다. 5.3 Mobilenet V3 DefinitionMobilenet V3 - Large 구조는 아래와 같다. (Mobilenet-Small은 위 Table 2 참고)* 6. Experiments 비교 대상: Accuracy vs. Latecny &amp; MAdds (Multiply adds) Training setup 학습 환경 : 4x4 TPU Pod Optimizer : RMSPropOptimizer with 0.9 Momentum (Tensorflow) Batch size: 4,096 (128 images per chip) Learning Rate 스케줄 : 0.01 initial rate of decaying 0.01 every 3 step dropout : 0.8 l2 weight decay 1e-5 Exponential moving Average with 0.9999 decay Batch Norm : average decay of 0.99 Latency Measurement Hardware: Google Pixel phones Framework: Tensorflow Lite Benchmark Tool Core 갯수: Single 6.1 Classification Dataset: ImageNet 6.2 Result Mobilenet V3-Large 1.0 성능(Accuracy) &gt; SOTA Mobile Device Target 모델 Mobilenet V3-Small 0.75 지연시간(Latency) &lt; SOTA Mobile Device Target 모델 이미지 해상도(Resolution)이 큰 Mobilenet-V3-Small이 Mobilenet-V3-Large보다 미세하게 성능이 우수함. 하지만, 이미지 해상도는 보통 결정된 값이기에 큰 의미는 없는듯 6.2.1 Ablation Study 전체 Network를 H-swish로 대체하면 성능-속도 trade-off가 좋지 않음 성능 증가 (0.9%) 속도 감소 (-32%) Network 뒷부분만 교체하면 성능-속도 trade-off가 조아짐 성능 증가(0.7%) 속도 감소 (-12%) 크게 의미 없는 수치같음… 6.2 Detection Datset: MsCOCO OD Architecture : SSD Lite (OS 16 ~ OS256) OS 16 (C4) MobilenetV3-Lage 13th bottleneck block MobileentV3-Small 9th bottleneck block OS 32 (C5) After pooling layer (Small &amp; Large) Reduced Channel into Half ImageNet의 클래스 수 (1,000)에 비해 MsCOCO의 클래스 수 (80)이 작기에 기존 channel 수는 redundunt하다고 여김 실제로 성능하락 없이 속도 향상(15%)만 가져옴 Mobilent V2와 동일한 성능 (22.0 mAP vs. 22.1 mAP)이면서 빠른 속도 (150 ms vs. 200 ms) 25% 속도 향상 보임 6.4 Semantic Segmentation Dataset : CityScapes Fine Images (mIOU) Pretraiend Model 사용 x Channel 절반 사용 (OD와 동일한 이유) CityScapes 클래스 19 &lt;&lt; ImageNet 클래스 1,000 LR-ASPP (Lite Reduced design of Atrous Spatial Pyramid Pooling module) 1x1 conv + Global Average Pooling Layer (+ Squeeze-and-Excite) Pooling Layer에 더 큰 stride, kernerl을 사용함 마지막 Layer에 Atrous conv.와 skip connection을 사용함. Mobilenet V3-Large: ESPNet v2, CCC2, ESPNetv1보다 각각 10.5%, 10.6%, 12.3% 성능 향상 + 무게 감소 (Madds) Mobilenet V3-Small: ESPNet v2, CCC2, ESPNetv1보다 각각 1.7, 1.59, 2.24 배 속도 향상 + 6.2% 성능 향상 7. Conclusion Classificaiton, Object Detection, Semantic Segmentation 모두 좋은 결과를 보임 Mobile model의 차세대 Network 제시 비선형 함수 h-swish 사용 양자화에 친화적인 Squeeze-and-Excite 사용 LR-ASPP 선보임.","link":"/2019/07/28/Mobilenetv3/"}],"tags":[],"categories":[]}