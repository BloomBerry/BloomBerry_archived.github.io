{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/07/28/hello-world/"},{"title":"Mobilenetv3","text":"Searching for Mobilenet V32019.05.09 Andrew Howard, et. al. from Google AI![](Screenshot from 2019-07-16 23-50-03.png) AbstractMobilenet V3는 hardware-aware NAS (Neural Architecture Search)기법인 NetAdapt을 사용하여 타겟하는 Mobile Device CPU에서 Latency를 줄임과 동시에 성능을 끌어올릴 수 있음을 보였다는데 의의가 있다. 본 논문에서는 Mobilenet V3를 Mobilenet V3-Large와 MobilenetV3-Small 두 가지 버전으로 나누어 Classification, Object Detection과 Semantic Segmentation을 수행하였다. Segmantic Segmentation의 경우는 LR-SPP (Lite Reduced-Spatial Pyramid Pooling)라는 새로운 Decoder구조를 제안하였다. 실험 결과 MobilenetV3-Lage의 Classification은 MobilenetV2에 비해 ImageNet에서 Top-1 accuracy가 3.2% 향상됨과 동시에 Latency가 15% 감소하였다.(MobilenetV3-Small은 accuracy 4.6% 감소) 1. Introduction이 논문은 모바일 디바이스 상에서 속도-성능간 trade-off를 최적화하기 위한 방안들을 다룬다. Complementary Search Engine : Latency &amp; Accuracy를 목표값으로 NAS (Neural Architecture Search)를 진행함. 모바일 셋팅에 최적화된 형태의 비선형 함수 H-swish 함수 사용 새로운 network 구조 제안 : Inverted Resudial + Squeeze-and-Excite 새로운 Segmentation Decoder (LR-SPP) 2. Related WorkAccuracy-Latency 최적화를 위한 다양한 연구 결과를 요약하면 다음과 같다. SqueezeNet : 1x1 conv로 parameter 수를 줄임 Mobilenet V1 : Depthwise Separable conv로 channel-wise 곱셈 paremeter 를 줄임 Mobilenet V2 : Inverted Residual과 Linear Bottleneck을 사용하여 모바일 디바이스 상에서 효율적인 구조 채택 ShuffleNet : Group conv + Channel shuffle을 사용하여 MAC 를 줄임 CondenseNet : Group Conv에 사용될 connections를 학습함 ShiftNet : Shift operation을 point-wise conv와 사용하여 계산량이 많은 Spatial conv 대체 3. Efficient Mobile Building BlocksMnasNet은 Mobilenet V2의 bottleneck 기본 뼈대에서 squeeze-and-excite구조와 light attention module을 추가한 구조를 채택하고 있다. Mobilenet V3는 MnasNet구조에서 swish non-linearity로 업데이트하였으며, fixed-point arithmetic구조에서 비효율적인 sigmoid를 hard sigmoid 구조를 채택하였다. ![](Screenshot from 2019-07-17 17-05-03.png) 4. Network SearchNetwork Search는 network의 최적 Block를 찾는데 Platform-aware NAS와 Layer 수 최적화를 위한 NetAdapt 알고리즘을 사용한다. 4.1 Platform-Aware NAS for Block-wise SearchPlatform-Aware NAS는 [43] MnasNet: Platform-Aware Nerual Architecture Search for Mobile 에서 다루고 있는 다중목적 보상함수를 사용하고 있다.$$ACC[m]×[LAT[m]/TAR]^w$$위 식은 Parto-optimal solution을 최적화하기 위해 Target Latency [TAR]항을 분모로 모델 m에 대하여 Accuracy [ACC[m]]와 Latency[LAT[m]] 항을 사용한다. *w은 ACC와 *LAT 간의 비율을 조정하는 항이다. 기존 논문 [43]에서는 w=-0.07를 사용하였지만, 본 논문에서는 작은 모델에 최적화하도록 LAT\\에 weight를 더 주기 위해 w=-0.15를 주어 모델 m을 찾는다. 4.2 NetAdapt for Layer-wise SearchNetAdapt는 *[46] Netadapt: Platform-Aware neural network adaptation for mobile applications*에서 다루고 있다. 요약하면, 4.1절에서 다중목적 보상 NAS로 찾은 seed architecture로 시작한다. 각 step에 대해서 새로운 proposal을 제안한다. 이때, proposal은 이전 step에서의 model에 비해 latency가 최소한계치 δ 만큼 감소시켜야 한다. δ=0.01L L= seed model의 Latency Pretrained weight 값으로 이전 step에서 사용한 model 중 truncated된 부분을 제외한 부분을 채택하며, 새로 생성된 layer는 randomly initialized시킨다. Proposal한 model들을 T step만큼 fine tuning을 거치고 accuracy를 생성한다. T=10,000를 사용 Evaluation Metric을 통해 proposal된 model들 중 최고의 proposal model을 찾는다. Eval Metric =max $$Δ|ACC|\\overΔLAT$$ Insight: Proposal이 discrete하기 때문에 trade-off curve slope를 최대화 함 Proposal 시 변경 가능한 요소들 Expansion Layer 크기 Bottleneck 크기 Target Latency에 도달할 때까지 1~2 과정을 반복한다. 5. Network ImprovementsNetwork Search로 찾은 모델을 더 개선하기 위해 계산량이 많은 Layer를 변경하고, Quantization frinedly한 H-swish 비선형 함수를 제안한다. 5.1 Redesigning Expensive LayersN기존 Network Search로 찾은 모델은 시작, 끝 Layer에서 유독 많은 계산량을 보였다. 이는 기존 Network Search Method의 search space를 벗어나는 부분으로, 수동 tuning을 하여 Accuracy는 불변하고, Latency를 줄여주었다. Final Layer ![](Screenshot from 2019-07-17 19-20-58.png) 7×7 Conv Layer into Average pooling Layer Same Accuracy 10 ms Latency (15% of running time) &amp; 30MAdds drop Initial Layer channel 32 channel into 16 channel Non-linearity Function : swish or ReLU Same Accuracy 3 ms Latency &amp; 10MAdds drop 5.2 Nonlinearitiesswish는 *[36] Searching for Activation Function*에서 제안한 비선형 함수로 ReLU를 대체하여 neural network의 accuracy를 향상시킨다. 이때, sigmoid σ(x)를 사용하기에 계산량의 증가가 있다.$$swish(x)=x⋅σ(x)$$sigmoid 함수 σ(x)의 계산량을 모바일 디바이스 상에서 줄이기 위한 두 가지 방법을 제안한다. h-swish ![](Screenshot from 2019-07-17 19-48-44.png)$$h−swish[x]=x\\frac{ReLU6(x+3)}6$$ ReLU6는 어떤 Hardware / Software platform에서도 최적화가 가능하다. Framework마다 Sigmoid 구현하는데 따른 numerical precision loss를 없앨 수 있다. Quantized Sigmoid조차도 ReLU보다 느리다. 위 세 가지 이유로, sigmoid 함수를 ReLU6로 바꾼다. h-swish 후 배치 비선형 함수로 인한 Latency는 뒤로 갈 수록 feature-map의 크기가 줄어듦에 따라 감소한다. 따라서, h-swish를 network의 뒤에만 배치하는 것이 더 효과적임을 실험적으로 구했다. ![](Screenshot from 2019-07-17 19-53-02.png) 위에 언급한 최적화를 사용함에도 불구하고, h-swish는 여전히 latency가 존재한다. 그럼에도 accuracy-latency tradeoff가 긍정적이고, 향후 software optimize에 따른 성능 향상을 염두하여 Network의 뒷부분에 H-swish 비선형 함수를 사용한다. ReLU와 같은 linear 함수 사용 시, latency bottleneck은 메모리 접근 시 발생함. 효과적으로 메모리 배치 시 최적화 가능함. 5.2.1. Large Squeeze-and-Excite MnasNet 논문에서는 Squeeze-and-excite의 channel은 convolution layer의 채널 수에 상대적이었다. 하지만, 본 논문에서는 Expansion Layer의 1/4 사이즈로 고정시켰다. 이로 인한 accuracy는 향상한 반면, parameter는 소폭만 증가헀다. 5.3 Mobilenet V3 DefinitionMobilenet V3 - Large 구조는 아래와 같다. (Mobilenet-Small은 위 Table 2 참고)* ![](Screenshot from 2019-07-17 19-52-54.png) 6. Experiments","link":"/2019/07/28/Mobilenetv3/"}],"tags":[],"categories":[]}